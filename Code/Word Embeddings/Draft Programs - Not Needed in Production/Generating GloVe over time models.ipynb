{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"Generating GloVe over time models.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"Z-acCjLbFeJt"},"source":["# Training Gensim model on neuroscience papers"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j_HTqeOlLwkp","executionInfo":{"status":"ok","timestamp":1634068245882,"user_tz":240,"elapsed":7945,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}},"outputId":"325b7806-f8d5-47ae-c121-fc31914c4a82"},"source":["!pip install python-docx"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting python-docx\n","  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n","\u001b[K     |████████████████████████████████| 5.6 MB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from python-docx) (4.2.6)\n","Building wheels for collected packages: python-docx\n","  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184508 sha256=0a176e8812e729d742df64d4e4c40ba974ffba4ae39004a9134369bcec11226f\n","  Stored in directory: /root/.cache/pip/wheels/f6/6f/b9/d798122a8b55b74ad30b5f52b01482169b445fbb84a11797a6\n","Successfully built python-docx\n","Installing collected packages: python-docx\n","Successfully installed python-docx-0.8.11\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"70HN_RS6T8OD","executionInfo":{"status":"ok","timestamp":1634068249229,"user_tz":240,"elapsed":3355,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}},"outputId":"7c99e144-b019-4ce2-8ea7-0afe366eec20"},"source":["!pip install glove_python-binary"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting glove_python-binary\n","  Downloading glove_python_binary-0.2.0-cp37-cp37m-manylinux1_x86_64.whl (948 kB)\n","\u001b[?25l\r\u001b[K     |▍                               | 10 kB 22.6 MB/s eta 0:00:01\r\u001b[K     |▊                               | 20 kB 28.7 MB/s eta 0:00:01\r\u001b[K     |█                               | 30 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 40 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 51 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██                              | 61 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 81 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |███                             | 92 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 174 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 184 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 194 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 204 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 215 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 225 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████                        | 235 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 245 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 256 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 266 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 276 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 286 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 296 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 307 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 317 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 327 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 337 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 348 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 358 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 368 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 378 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 389 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 399 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 409 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 419 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 430 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 440 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 450 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 460 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 471 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 481 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 491 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 501 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 512 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 522 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 532 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 542 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 552 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 563 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 573 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 583 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 593 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 604 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 614 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 624 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 634 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 645 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 655 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 665 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 675 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 686 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 696 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 706 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 716 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 727 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 737 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 747 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 757 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 768 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 778 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 788 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 798 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 808 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 819 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 829 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 839 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 849 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 860 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 870 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 880 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 890 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 901 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 911 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 921 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 931 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 942 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 948 kB 5.3 MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from glove_python-binary) (1.4.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from glove_python-binary) (1.19.5)\n","Installing collected packages: glove-python-binary\n","Successfully installed glove-python-binary-0.2.0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CYDKvAMYLy4s","executionInfo":{"status":"ok","timestamp":1634068251411,"user_tz":240,"elapsed":2185,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}},"outputId":"30b9c96c-24c7-44e5-801c-3d0e200c1fcd"},"source":["from docx import Document\n","import nltk\n","nltk.download('punkt')\n","import re\n","from nltk import sent_tokenize\n","import pandas as pd\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","import pickle\n","import numpy as np\n","import glob"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"code","metadata":{"id":"BDPlGB-qwa6Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634068251779,"user_tz":240,"elapsed":371,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}},"outputId":"e9114952-2b66-4c9d-f952-a78ce9b52a1d"},"source":["from nltk.stem import WordNetLemmatizer\n","import nltk \n","nltk.download('wordnet')"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"F6wvdOKbdf-L","executionInfo":{"status":"ok","timestamp":1634075018953,"user_tz":240,"elapsed":136,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}}},"source":["from nltk.stem.porter import PorterStemmer"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z3q0BCLUETkG","executionInfo":{"status":"ok","timestamp":1634068413901,"user_tz":240,"elapsed":121,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}}},"source":["from glove import Corpus, Glove"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qJqALNlWL07y","executionInfo":{"status":"ok","timestamp":1634068252693,"user_tz":240,"elapsed":817,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}},"outputId":"3337a9bd-fdfb-4261-aa83-a5ed6276fcb2"},"source":["!git clone 'https://github.com/igorbrigadir/stopwords.git'"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'stopwords'...\n","remote: Enumerating objects: 149, done.\u001b[K\n","remote: Total 149 (delta 0), reused 0 (delta 0), pack-reused 149\u001b[K\n","Receiving objects: 100% (149/149), 85.27 KiB | 1.20 MiB/s, done.\n","Resolving deltas: 100% (52/52), done.\n"]}]},{"cell_type":"code","metadata":{"id":"JhyPjMvkL3fb","executionInfo":{"status":"ok","timestamp":1634068252694,"user_tz":240,"elapsed":12,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}}},"source":["alir3z4_data = '/content/stopwords/en/alir3z4.txt'\n","\n","more_stops = pd.read_csv('/content/stopwords/en/alir3z4.txt')\n","new_stops = list(more_stops[\"'ll\"])"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"2QIGTVVjMGw3","executionInfo":{"status":"ok","timestamp":1634068252695,"user_tz":240,"elapsed":12,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}}},"source":["DOMAIN_STOPS = {'pubmed', 'et', 'al', 'page'}\n","STOPWORDS =  set(stopwords.words('english') + stopwords.words('german') +  stopwords.words('dutch') + stopwords.words('french') +  stopwords.words('spanish')  + new_stops) | DOMAIN_STOPS\n","STOPWORDS = set(STOPWORDS)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C8pcXJs1MJXa","executionInfo":{"status":"ok","timestamp":1634068252696,"user_tz":240,"elapsed":12,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}},"outputId":"91f63e2b-35a7-4593-9347-70f90e2ea2f6"},"source":["len(STOPWORDS)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2011"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"XAIUEHTcyP4K","executionInfo":{"status":"ok","timestamp":1634068252696,"user_tz":240,"elapsed":8,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}}},"source":["ROOT = \"/content/drive/MyDrive/regen_x\""],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"CohG2_FkMKh5","executionInfo":{"status":"ok","timestamp":1634068254728,"user_tz":240,"elapsed":2039,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}}},"source":["def get_docx(file_path):\n","    doc = []\n","    for para in Document(file_path).paragraphs:\n","        if para.text == \"\":\n","            continue\n","        doc += (sent_tokenize(para.text.lower())) # we lower text here\n","    return doc\n","\n","\n","def get_start_stop():\n","    domain_stops = {'pubmed', 'et', 'al', 'page'}\n","    with open('/content/stopwords/en/alir3z4.txt', 'r') as fn:\n","        new_stops = [line.strip() for line in fn.readlines()]\n","    STOPWORDS =  set(stopwords.words('english') + stopwords.words('german') +  stopwords.words('dutch') + stopwords.words('french') +  stopwords.words('spanish')  + new_stops) | domain_stops\n","\n","    fn = glob.glob(ROOT + '/data/start-words/*')\n","    ALL_STARTS = [pickle.load(open(f , 'rb')) for f in fn]\n","    STARTWORDS = {}\n","    for f in ALL_STARTS:\n","      STARTWORDS.update(f)\n","    STARTWORDS = set(STARTWORDS.keys())\n","\n","    assert(type(STOPWORDS)==set and type(STARTWORDS)==set)\n","    return (STARTWORDS, STOPWORDS)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZoJbCIdJVwBq","executionInfo":{"status":"ok","timestamp":1634068254729,"user_tz":240,"elapsed":7,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}}},"source":["STARTWORDS, STOPWORDS = get_start_stop()"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K0rsrAgqMRJm"},"source":["# Optimizing Training Function"]},{"cell_type":"markdown","metadata":{"id":"KFddKJvTSFMi"},"source":["You **don't** want to do incremental training for the reasons given in [this answer](https://stackoverflow.com/questions/42746007/incremental-word2vec-model-training-in-gensim)"]},{"cell_type":"markdown","metadata":{"id":"34kR_V0rafxd"},"source":["# Time Period Binning"]},{"cell_type":"code","metadata":{"id":"3wm2DqshdhtV","executionInfo":{"status":"ok","timestamp":1634068254729,"user_tz":240,"elapsed":6,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}}},"source":["from natsort import natsorted\n","import os"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"wDuMaDxdcrws","executionInfo":{"status":"ok","timestamp":1634068254730,"user_tz":240,"elapsed":6,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}}},"source":["ROOT = \"/content/drive/MyDrive/regen_x\"\n","NUM_BINS = 16"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"iMOnwA-Yajfn","executionInfo":{"status":"ok","timestamp":1634068254730,"user_tz":240,"elapsed":5,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}}},"source":["def get_time_per_list(NUM_BINS):\n","    all_paths = natsorted(glob.glob(ROOT + '/data/ocr_paper_COMPREHENSIVE/*/'))\n","    all_path_chunked = np.array_split(all_paths , NUM_BINS)\n","    all_paths = np.array_split(all_paths , NUM_BINS)\n","    time_periods = {}\n","    time_per_list = []\n","    for i , file_chunk in enumerate(all_paths):\n","        time_periods[i] = file_chunk\n","        for j in range(len(time_periods[i])):\n","            time_periods[i][j] = time_periods[i][j].split('/')[-2]\n","        time_periods[i] = str(time_periods[i][0]) + '-' +  str(time_periods[i][-1])\n","        time_per_list.append(time_periods[i])\n","    return (time_per_list, all_path_chunked)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"fpPHYlb2db3R","executionInfo":{"status":"ok","timestamp":1634068297214,"user_tz":240,"elapsed":6859,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}}},"source":["(time_per_list, all_path_chunked) = get_time_per_list(NUM_BINS)\n","(STOPWORDS, STARTWORDS) = get_start_stop()"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wzwD5YgRjwKt","executionInfo":{"status":"ok","timestamp":1634068298565,"user_tz":240,"elapsed":104,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}},"outputId":"0ae4faf3-cc98-4274-9ff6-87bbc7832a5e"},"source":["all_path_chunked[0]"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['/content/drive/MyDrive/regen_x/data/ocr_paper_COMPREHENSIVE/1776/',\n","       '/content/drive/MyDrive/regen_x/data/ocr_paper_COMPREHENSIVE/1795/',\n","       '/content/drive/MyDrive/regen_x/data/ocr_paper_COMPREHENSIVE/1820/',\n","       '/content/drive/MyDrive/regen_x/data/ocr_paper_COMPREHENSIVE/1824/',\n","       '/content/drive/MyDrive/regen_x/data/ocr_paper_COMPREHENSIVE/1826/',\n","       '/content/drive/MyDrive/regen_x/data/ocr_paper_COMPREHENSIVE/1827/',\n","       '/content/drive/MyDrive/regen_x/data/ocr_paper_COMPREHENSIVE/1828/',\n","       '/content/drive/MyDrive/regen_x/data/ocr_paper_COMPREHENSIVE/1831/',\n","       '/content/drive/MyDrive/regen_x/data/ocr_paper_COMPREHENSIVE/1832/'],\n","      dtype='<U65')"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"JetqIUCinAE8","executionInfo":{"status":"ok","timestamp":1634068735241,"user_tz":240,"elapsed":1097,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}}},"source":["# We need to split the document into sentences. \n","# Then concatenate all the documents of a given year into one big array with all their sentences. \n","\n","# This functions takes a folder of files and returns one array with \n","# all of the files processed sentences(which themselves are a list of words) as elements \n","def get_proc_docs(year_path, STARTWORDS, STOPWORDS, max_papers=None, verbose=True, use_porter=False, useStopWords=True):\n","  file_paths = glob.glob(year_path + \"*.docx\")\n","\n","  print(\"Number of files: {}\".format(len(file_paths)))\n","  if len(file_paths) == 0:\n","    # raise Exception(\"Folder has no files - maybe drive was not mounted?\")\n","    pass \n","  ## -- Collecting Papers from Given Year -- ##\n","  proc_docs = [] \n","\n","  counter = 1\n","  length = len(file_paths)\n","  for f in file_paths:\n","    doc = get_docx(f)\n","    \n","    for sentence in doc:\n","      # don't think we need to remove stopwords and such if we're training embeddings \n","      # do lemmatization here as well \n","\n","      proc_sentence = [] \n","      if useStopWords:\n","        proc_sentence = [word for word in re.findall(r'\\w+', sentence) if ((len(word) > 2) and (word not in STOPWORDS))]\n","      else:\n","        proc_sentence = [word for word in re.findall(r'\\w+', sentence)]\n","\n","      if use_porter:\n","        proc_sentence = do_stemming(proc_sentence) \n","      else:\n","        proc_sentence = do_lemmatizing(proc_sentence) \n","      proc_docs.append(proc_sentence)  \n","\n","    if(verbose):\n","      print(\"\\t{}/{}\".format(counter, length))\n","    counter += 1\n","\n","    if max_papers != None:\n","      if counter == max_papers+1:\n","        break \n","\n","  return proc_docs\n","\n","def do_stemming(filtered):\n","\tstemmed = []\n","\tfor f in filtered:\n","\t\tstemmed.append(PorterStemmer().stem(f))\n","\t\t#stemmed.append(LancasterStemmer().stem(f))\n","\t\t#stemmed.append(SnowballStemmer('english').stem(f))\n","\treturn stemmed\n","\n","# for lemmatization \n","import spacy\n","# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n","nlp = spacy.load('en', disable=['parser', 'ner'])\n","\n","def do_lemmatizing(filtered):\n","  # convert list to string \n","  spacy_parsed_text = nlp(\" \".join(filtered)) \n","  # Get the lemma for each token in the parsed text \n","  \n","  # I wanted to keep pronouns so not taking lemma if it's a pronoun but if you want to remove pronouns use below commented line \n","  # return \" \".join([token.lemma_ for token in doc])\n","\n","  # return as list of words again \n","  return [token.lemma_ if token.lemma_ != '-PRON-' else token.lower_ for token in spacy_parsed_text]"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"8jsJkCu7bRDe","executionInfo":{"status":"ok","timestamp":1634074497846,"user_tz":240,"elapsed":109,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}}},"source":["# This functions takes a folder of files and returns one array with \n","# all of the files processed sentences(which themselves are a list of words) as elements \n","def get_proc_docs_glove(year_path, STARTWORDS, STOPWORDS, max_papers=None, verbose=True, use_porter=False, useStopWords=True):\n","  file_paths = glob.glob(year_path + \"*.docx\")\n","\n","\n","  print(\"Number of files: {}\".format(len(file_paths)))\n","  if len(file_paths) == 0:\n","    # raise Exception(\"Folder has no files - maybe drive was not mounted?\")\n","    pass \n","  ## -- Collecting Papers from Given Year -- ##\n","  proc_docs = [] \n","\n","  counter = 1\n","  length = len(file_paths)\n","  for f in file_paths:\n","    doc = ' '.join(get_docx(f))\n","    # proc_doc = [word for word in re.findall(r'\\w+', doc.lower()) if ((word in STARTWORDS) and (len(word) > 2) and (word not in STOPWORDS))]\n","    proc_doc = [] \n","    if useStopWords:\n","      proc_doc = [word for word in re.findall(r'\\w+', doc) if ((len(word) > 2) and (word not in STOPWORDS))]\n","    else:\n","      proc_doc = [word for word in re.findall(r'\\w+', doc)]\n","\n","    if use_porter:\n","      proc_doc = do_stemming(proc_doc)      \n","    else:\n","      proc_doc = do_lemmatizing(proc_doc)\n","    proc_docs.append(proc_doc)\n","    print(\"{}/{}\".format(counter, length))\n","    counter += 1\n","\n","    if max_papers != None:\n","      if counter == max_papers+1:\n","        break \n","\n","  return proc_docs"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"_TiMTUOJB-mG","executionInfo":{"status":"ok","timestamp":1634068304618,"user_tz":240,"elapsed":110,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}}},"source":["def train_glove(proc_docs):\n","  #Creating a corpus object\n","  corpus = Corpus() \n","\n","  #Training the corpus to generate the co occurence matrix which is used in GloVe\n","  corpus.fit(proc_docs, window=10)\n","\n","  glove = Glove(no_components=5, learning_rate=0.05) \n","  glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)\n","  glove.add_dictionary(corpus.dictionary)\n","  # glove.save('glove.model')\n","\n","  return glove "],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"rtCma8JXDVEg","executionInfo":{"status":"ok","timestamp":1634074520790,"user_tz":240,"elapsed":187,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}}},"source":["MODEL_PATH = \"/content/drive/MyDrive/Colab Notebooks/NLP - Lab/WordEmbeddings/Models/GloVe/\"\n","\n","if not os.path.exists(MODEL_PATH):\n","    os.makedirs(MODEL_PATH)"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"xGLmFO4Bdcz9","executionInfo":{"status":"error","timestamp":1634076949737,"user_tz":240,"elapsed":282851,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}},"outputId":"b6839ee3-5d1e-498e-974f-04b504956af7"},"source":["### Train and Save Models for all time periods ##\n","\n","counter = 0 \n","for time_period_paths in all_path_chunked:\n","  cur_time_period = time_per_list[counter] \n","  print(\"Current Time Period: {}\".format(cur_time_period))\n","  all_proc_docs_time_period = [] \n","\n","  for i, year_path in enumerate(time_period_paths):\n","    print(\"{}/{}\".format(i+1, len(time_period_paths)))\n","\n","    proc_doc_cur_year = get_proc_docs_glove(year_path, STARTWORDS, STOPWORDS, max_papers=None, verbose=True, use_porter=True, useStopWords=True) \n","\n","    all_proc_docs_time_period += all_proc_docs_time_period + proc_doc_cur_year \n","\n","  # Train the embeddings! \n","  print(\"Training word embeddings for {}...\".format(cur_time_period))\n","  model = train_glove(all_proc_docs_time_period) \n","\n","  try:\n","    print(model.most_similar(\"eye\", number=10))\n","  except:\n","    print(\"Not in vocab\")\n","  # Store just the words + their trained embeddings.\n","  # with open(MODEL_PATH + \"GloVe_Stemmed/{}_Stemmed.txt\".format(cur_time_period), \"w\") as f:\n","  #   for word in model.dictionary:\n","  #       f.write(word)\n","  #       f.write(\" \")\n","  #       for i in range(0, 5):\n","  #           f.write(str(model.word_vectors[model.dictionary[word]][i]))\n","  #           f.write(\" \")\n","  #       f.write(\"\\n\")\n","\n","  # Manually create space \n","  del all_proc_docs_time_period\n","  del model \n","\n","  counter += 1"],"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["Current Time Period: 1776-1832\n","1/9\n","Number of files: 1\n","1/1\n","2/9\n","Number of files: 1\n","1/1\n","3/9\n","Number of files: 1\n","1/1\n","4/9\n","Number of files: 2\n","1/2\n","2/2\n","5/9\n","Number of files: 1\n","1/1\n","6/9\n","Number of files: 1\n","1/1\n","7/9\n","Number of files: 2\n","1/2\n","2/2\n","8/9\n","Number of files: 1\n","1/1\n","9/9\n","Number of files: 2\n","1/2\n","2/2\n","Training word embeddings for 1776-1832...\n","Performing 30 training epochs with 4 threads\n","Epoch 0\n","Epoch 1\n","Epoch 2\n","Epoch 3\n","Epoch 4\n","Epoch 5\n","Epoch 6\n","Epoch 7\n","Epoch 8\n","Epoch 9\n","Epoch 10\n","Epoch 11\n","Epoch 12\n","Epoch 13\n","Epoch 14\n","Epoch 15\n","Epoch 16\n","Epoch 17\n","Epoch 18\n","Epoch 19\n","Epoch 20\n","Epoch 21\n","Epoch 22\n","Epoch 23\n","Epoch 24\n","Epoch 25\n","Epoch 26\n","Epoch 27\n","Epoch 28\n","Epoch 29\n","Not in vocab\n","Current Time Period: 1835-1846\n","1/9\n","Number of files: 2\n","1/2\n","2/2\n","2/9\n","Number of files: 1\n","1/1\n","3/9\n","Number of files: 0\n","4/9\n","Number of files: 1\n","1/1\n","5/9\n","Number of files: 1\n","1/1\n","6/9\n","Number of files: 1\n","1/1\n","7/9\n","Number of files: 1\n","1/1\n","8/9\n","Number of files: 1\n","1/1\n","9/9\n","Number of files: 2\n","1/2\n","2/2\n","Training word embeddings for 1835-1846...\n","Performing 30 training epochs with 4 threads\n","Epoch 0\n","Epoch 1\n","Epoch 2\n","Epoch 3\n","Epoch 4\n","Epoch 5\n","Epoch 6\n","Epoch 7\n","Epoch 8\n","Epoch 9\n","Epoch 10\n","Epoch 11\n","Epoch 12\n","Epoch 13\n","Epoch 14\n","Epoch 15\n","Epoch 16\n","Epoch 17\n","Epoch 18\n","Epoch 19\n","Epoch 20\n","Epoch 21\n","Epoch 22\n","Epoch 23\n","Epoch 24\n","Epoch 25\n","Epoch 26\n","Epoch 27\n","Epoch 28\n","Epoch 29\n","[('erythrophthalmu', 0.9990000067996806), ('blenni', 0.9987386372988325), ('gadu', 0.998598127611439), ('abnorm', 0.9975330137783431), ('hold', 0.9975145863810408), ('swim', 0.9974818052174755), ('cyprinu', 0.9974237723330167), ('ticabl', 0.9968540640093145), ('timesin', 0.9966782257951488)]\n","Current Time Period: 1847-1859\n","1/8\n","Number of files: 0\n","2/8\n","Number of files: 3\n","1/3\n","2/3\n","3/3\n","3/8\n","Number of files: 1\n","1/1\n","4/8\n","Number of files: 1\n","1/1\n","5/8\n","Number of files: 1\n","1/1\n","6/8\n","Number of files: 2\n","1/2\n","2/2\n","7/8\n","Number of files: 2\n","1/2\n","2/2\n","8/8\n","Number of files: 3\n","1/3\n","2/3\n","3/3\n","Training word embeddings for 1847-1859...\n","Performing 30 training epochs with 4 threads\n","Epoch 0\n","Epoch 1\n","Epoch 2\n","Epoch 3\n","Epoch 4\n","Epoch 5\n","Epoch 6\n","Epoch 7\n","Epoch 8\n","Epoch 9\n","Epoch 10\n","Epoch 11\n","Epoch 12\n","Epoch 13\n","Epoch 14\n","Epoch 15\n","Epoch 16\n","Epoch 17\n","Epoch 18\n","Epoch 19\n","Epoch 20\n","Epoch 21\n","Epoch 22\n","Epoch 23\n","Epoch 24\n","Epoch 25\n","Epoch 26\n","Epoch 27\n","Epoch 28\n","Epoch 29\n","Not in vocab\n","Current Time Period: 1860-1872\n","1/8\n","Number of files: 2\n","1/2\n","2/2\n","2/8\n","Number of files: 1\n","1/1\n","3/8\n","Number of files: 2\n","1/2\n","2/2\n","4/8\n","Number of files: 1\n","1/1\n","5/8\n","Number of files: 3\n","1/3\n","2/3\n","3/3\n","6/8\n","Number of files: 1\n","1/1\n","7/8\n","Number of files: 1\n","1/1\n","8/8\n","Number of files: 0\n","Training word embeddings for 1860-1872...\n","Performing 30 training epochs with 4 threads\n","Epoch 0\n","Epoch 1\n","Epoch 2\n","Epoch 3\n","Epoch 4\n","Epoch 5\n","Epoch 6\n","Epoch 7\n","Epoch 8\n","Epoch 9\n","Epoch 10\n","Epoch 11\n","Epoch 12\n","Epoch 13\n","Epoch 14\n","Epoch 15\n","Epoch 16\n","Epoch 17\n","Epoch 18\n","Epoch 19\n","Epoch 20\n","Epoch 21\n","Epoch 22\n","Epoch 23\n","Epoch 24\n","Epoch 25\n","Epoch 26\n","Epoch 27\n","Epoch 28\n","Epoch 29\n","Not in vocab\n","Current Time Period: 1874-1884\n","1/8\n","Number of files: 2\n","1/2\n","2/2\n","2/8\n","Number of files: 1\n","1/1\n","3/8\n","Number of files: 1\n","1/1\n","4/8\n","Number of files: 2\n","1/2\n","2/2\n","5/8\n","Number of files: 1\n","1/1\n","6/8\n","Number of files: 1\n","1/1\n","7/8\n","Number of files: 4\n","1/4\n","2/4\n","3/4\n","4/4\n","8/8\n","Number of files: 3\n","1/3\n","2/3\n","3/3\n","Training word embeddings for 1874-1884...\n","Performing 30 training epochs with 4 threads\n","Epoch 0\n","Epoch 1\n","Epoch 2\n","Epoch 3\n","Epoch 4\n","Epoch 5\n","Epoch 6\n","Epoch 7\n","Epoch 8\n","Epoch 9\n","Epoch 10\n","Epoch 11\n","Epoch 12\n","Epoch 13\n","Epoch 14\n","Epoch 15\n","Epoch 16\n","Epoch 17\n","Epoch 18\n","Epoch 19\n","Epoch 20\n","Epoch 21\n","Epoch 22\n","Epoch 23\n","Epoch 24\n","Epoch 25\n","Epoch 26\n","Epoch 27\n","Epoch 28\n","Epoch 29\n","Not in vocab\n","Current Time Period: 1885-1894\n","1/8\n","Number of files: 2\n","1/2\n","2/2\n","2/8\n","Number of files: 1\n","1/1\n","3/8\n","Number of files: 2\n","1/2\n","2/2\n","4/8\n","Number of files: 1\n","1/1\n","5/8\n","Number of files: 1\n","1/1\n","6/8\n","Number of files: 2\n","1/2\n","2/2\n","7/8\n","Number of files: 1\n","1/1\n","8/8\n","Number of files: 1\n","1/1\n","Training word embeddings for 1885-1894...\n","Performing 30 training epochs with 4 threads\n","Epoch 0\n","Epoch 1\n","Epoch 2\n","Epoch 3\n","Epoch 4\n","Epoch 5\n","Epoch 6\n","Epoch 7\n","Epoch 8\n","Epoch 9\n","Epoch 10\n","Epoch 11\n","Epoch 12\n","Epoch 13\n","Epoch 14\n","Epoch 15\n","Epoch 16\n","Epoch 17\n","Epoch 18\n","Epoch 19\n","Epoch 20\n","Epoch 21\n","Epoch 22\n","Epoch 23\n","Epoch 24\n","Epoch 25\n","Epoch 26\n","Epoch 27\n","Epoch 28\n","Epoch 29\n","Not in vocab\n","Current Time Period: 1895-1902\n","1/8\n","Number of files: 2\n","1/2\n","2/2\n","2/8\n","Number of files: 3\n","1/3\n","2/3\n","3/3\n","3/8\n","Number of files: 4\n","1/4\n","2/4\n","3/4\n","4/4\n","4/8\n","Number of files: 3\n","1/3\n","2/3\n","3/3\n","5/8\n","Number of files: 2\n","1/2\n","2/2\n","6/8\n","Number of files: 1\n","1/1\n","7/8\n","Number of files: 2\n","1/2\n","2/2\n","8/8\n","Number of files: 3\n","1/3\n","2/3\n","3/3\n","Training word embeddings for 1895-1902...\n","Performing 30 training epochs with 4 threads\n","Epoch 0\n","Epoch 1\n","Epoch 2\n","Epoch 3\n","Epoch 4\n","Epoch 5\n","Epoch 6\n","Epoch 7\n","Epoch 8\n","Epoch 9\n","Epoch 10\n","Epoch 11\n","Epoch 12\n","Epoch 13\n","Epoch 14\n","Epoch 15\n","Epoch 16\n","Epoch 17\n","Epoch 18\n","Epoch 19\n","Epoch 20\n","Epoch 21\n","Epoch 22\n","Epoch 23\n","Epoch 24\n","Epoch 25\n","Epoch 26\n","Epoch 27\n","Epoch 28\n","Epoch 29\n","Not in vocab\n","Current Time Period: 1903-1910\n","1/8\n","Number of files: 1\n","1/1\n","2/8\n","Number of files: 1\n","1/1\n","3/8\n","Number of files: 3\n","1/3\n","2/3\n","3/3\n","4/8\n","Number of files: 5\n","1/5\n","2/5\n","3/5\n","4/5\n","5/5\n","5/8\n","Number of files: 5\n","1/5\n","2/5\n","3/5\n","4/5\n","5/5\n","6/8\n","Number of files: 11\n","1/11\n","2/11\n","3/11\n","4/11\n","5/11\n","6/11\n","7/11\n","8/11\n","9/11\n","10/11\n","11/11\n","7/8\n","Number of files: 5\n","1/5\n","2/5\n","3/5\n","4/5\n","5/5\n","8/8\n","Number of files: 6\n","1/6\n","2/6\n","3/6\n","4/6\n","5/6\n","6/6\n","Training word embeddings for 1903-1910...\n","Performing 30 training epochs with 4 threads\n","Epoch 0\n","Epoch 1\n","Epoch 2\n","Epoch 3\n","Epoch 4\n","Epoch 5\n","Epoch 6\n","Epoch 7\n","Epoch 8\n","Epoch 9\n","Epoch 10\n","Epoch 11\n","Epoch 12\n","Epoch 13\n","Epoch 14\n","Epoch 15\n","Epoch 16\n","Epoch 17\n","Epoch 18\n","Epoch 19\n","Epoch 20\n","Epoch 21\n","Epoch 22\n","Epoch 23\n","Epoch 24\n","Epoch 25\n","Epoch 26\n","Epoch 27\n","Epoch 28\n","Epoch 29\n","[('irrat', 0.9980190259200011), ('oosoicn', 0.9974173789861709), ('bryant', 0.9965069661207678), ('intramusculair', 0.9962480898753804), ('pride', 0.9954768250784379), ('pfliiger', 0.9949148652362422), ('unabhangig', 0.9947558476318218), ('ososs', 0.9943875214083245), ('sell', 0.9939294195587632)]\n","Current Time Period: 1911-1918\n","1/8\n","Number of files: 5\n","1/5\n","2/5\n","3/5\n","4/5\n","5/5\n","2/8\n","Number of files: 6\n","1/6\n","2/6\n","3/6\n","4/6\n","5/6\n","6/6\n","3/8\n","Number of files: 10\n","1/10\n","2/10\n","3/10\n","4/10\n","5/10\n","6/10\n","7/10\n","8/10\n","9/10\n","10/10\n","4/8\n","Number of files: 7\n","1/7\n","2/7\n","3/7\n","4/7\n","5/7\n","6/7\n","7/7\n","5/8\n","Number of files: 8\n","1/8\n","2/8\n","3/8\n","4/8\n","5/8\n","6/8\n","7/8\n","8/8\n","6/8\n","Number of files: 5\n","1/5\n","2/5\n","3/5\n","4/5\n","5/5\n","7/8\n","Number of files: 2\n","1/2\n","2/2\n","8/8\n","Number of files: 5\n","1/5\n","2/5\n","3/5\n","4/5\n","5/5\n","Training word embeddings for 1911-1918...\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-44-80da49e9147d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;31m# Train the embeddings!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training word embeddings for {}...\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_time_period\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_glove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_proc_docs_time_period\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-b7c7be001fe6>\u001b[0m in \u001b[0;36mtrain_glove\u001b[0;34m(proc_docs)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m#Training the corpus to generate the co occurence matrix which is used in GloVe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mglove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGlove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/glove/corpus.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, corpus, window, ignore_missing)\u001b[0m\n\u001b[1;32m     62\u001b[0m                                                     \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary_supplied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                                                     \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                                                     int(ignore_missing))\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mglove\\corpus_cython.pyx\u001b[0m in \u001b[0;36mglove.corpus_cython.construct_cooccurrence_matrix\u001b[0;34m()\u001b[0m\n","\u001b[0;32mglove\\corpus_cython.pyx\u001b[0m in \u001b[0;36mglove.corpus_cython.matrix_to_coo\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/sparse/coo.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'coo'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0m_data_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"uIPvngFbowMh","executionInfo":{"status":"ok","timestamp":1634076219589,"user_tz":240,"elapsed":108,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}}},"source":["# Exceution of above function took: 19 mins and 42 seconds"],"execution_count":38,"outputs":[]},{"cell_type":"code","metadata":{"id":"zwK59B1x0DvC","executionInfo":{"status":"aborted","timestamp":1634068254883,"user_tz":240,"elapsed":16,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}}},"source":[""],"execution_count":null,"outputs":[]}]}