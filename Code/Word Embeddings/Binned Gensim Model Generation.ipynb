{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"provenance":[]}},"cells":[{"cell_type":"markdown","source":["- This was done using gensim with lemmatization and stopwords still included as that was found to be the best method\n","- Trained Gensim model over time period binning as descriped in RegenX paper and saves model for further analysis"],"metadata":{"id":"d7HPG8dNRSnp"}},{"cell_type":"markdown","metadata":{"id":"Z-acCjLbFeJt"},"source":["# Training Gensim model on neuroscience papers"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j_HTqeOlLwkp","executionInfo":{"status":"ok","timestamp":1634079791348,"user_tz":240,"elapsed":3156,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}},"outputId":"cdef6864-e9bc-4532-90a7-7044bc8cb704"},"source":["!pip install python-docx"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: python-docx in /usr/local/lib/python3.7/dist-packages (0.8.11)\n","Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from python-docx) (4.2.6)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"70HN_RS6T8OD","executionInfo":{"status":"ok","timestamp":1634079794445,"user_tz":240,"elapsed":3100,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}},"outputId":"4b8c9aed-4fc1-41f4-ff69-781177f30d17"},"source":["!pip install glove_python-binary"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: glove_python-binary in /usr/local/lib/python3.7/dist-packages (0.2.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from glove_python-binary) (1.19.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from glove_python-binary) (1.4.1)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CYDKvAMYLy4s","executionInfo":{"status":"ok","timestamp":1634079794446,"user_tz":240,"elapsed":31,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}},"outputId":"665addf8-94cf-4b4e-d91b-011ec15f4ad4"},"source":["from docx import Document\n","import nltk\n","nltk.download('punkt')\n","import re\n","from nltk import sent_tokenize\n","import pandas as pd\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","import pickle\n","import numpy as np\n","import glob"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"code","metadata":{"id":"BDPlGB-qwa6Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634079794447,"user_tz":240,"elapsed":18,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}},"outputId":"5f7dfdd2-fe8b-4ce7-e617-010c24f70c68"},"source":["from nltk.stem import WordNetLemmatizer\n","import nltk \n","nltk.download('wordnet')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qJqALNlWL07y","executionInfo":{"status":"ok","timestamp":1634079794642,"user_tz":240,"elapsed":204,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}},"outputId":"87f4bfbb-01b8-4858-e88e-27d70f753bd2"},"source":["!git clone 'https://github.com/igorbrigadir/stopwords.git'"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'stopwords' already exists and is not an empty directory.\n"]}]},{"cell_type":"code","metadata":{"id":"JhyPjMvkL3fb"},"source":["alir3z4_data = '/content/stopwords/en/alir3z4.txt'\n","\n","more_stops = pd.read_csv('/content/stopwords/en/alir3z4.txt')\n","new_stops = list(more_stops[\"'ll\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2QIGTVVjMGw3"},"source":["DOMAIN_STOPS = {'pubmed', 'et', 'al', 'page'}\n","STOPWORDS =  set(stopwords.words('english') + stopwords.words('german') +  stopwords.words('dutch') + stopwords.words('french') +  stopwords.words('spanish')  + new_stops) | DOMAIN_STOPS\n","STOPWORDS = set(STOPWORDS)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C8pcXJs1MJXa","executionInfo":{"status":"ok","timestamp":1634079794644,"user_tz":240,"elapsed":14,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}},"outputId":"2e7974f1-c416-49a2-972c-4e83d7b6fa25"},"source":["len(STOPWORDS)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2011"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"-84pfgeZvXIK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634079794644,"user_tz":240,"elapsed":11,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}},"outputId":"3355cfe2-32d8-42cf-fee9-d812fc207769"},"source":["'a' in STOPWORDS"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"XAIUEHTcyP4K"},"source":["ROOT = \"/content/drive/MyDrive/regen_x\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CohG2_FkMKh5"},"source":["def get_docx(file_path):\n","    doc = []\n","    for para in Document(file_path).paragraphs:\n","        if para.text == \"\":\n","            continue\n","        doc += (sent_tokenize(para.text.lower())) # we lower text here\n","    return doc\n","\n","\n","def get_start_stop():\n","    domain_stops = {'pubmed', 'et', 'al', 'page'}\n","    with open('/content/stopwords/en/alir3z4.txt', 'r') as fn:\n","        new_stops = [line.strip() for line in fn.readlines()]\n","    STOPWORDS =  set(stopwords.words('english') + stopwords.words('german') +  stopwords.words('dutch') + stopwords.words('french') +  stopwords.words('spanish')  + new_stops) | domain_stops\n","\n","    fn = glob.glob(ROOT + '/data/start-words/*')\n","    ALL_STARTS = [pickle.load(open(f , 'rb')) for f in fn]\n","    STARTWORDS = {}\n","    for f in ALL_STARTS:\n","      STARTWORDS.update(f)\n","    STARTWORDS = set(STARTWORDS.keys())\n","\n","    assert(type(STOPWORDS)==set and type(STARTWORDS)==set)\n","    return (STARTWORDS, STOPWORDS)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZoJbCIdJVwBq"},"source":["STARTWORDS, STOPWORDS = get_start_stop()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"82OoKfEym1uK"},"source":["from gensim.models import Word2Vec"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K0rsrAgqMRJm"},"source":["# Optimizing Training Function"]},{"cell_type":"markdown","metadata":{"id":"KFddKJvTSFMi"},"source":["You **don't** want to do incremental training for the reasons given in [this answer](https://stackoverflow.com/questions/42746007/incremental-word2vec-model-training-in-gensim)"]},{"cell_type":"markdown","metadata":{"id":"34kR_V0rafxd"},"source":["# Time Period Binning"]},{"cell_type":"code","metadata":{"id":"3wm2DqshdhtV"},"source":["from natsort import natsorted\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wDuMaDxdcrws"},"source":["ROOT = \"/content/drive/MyDrive/regen_x\"\n","NUM_BINS = 16"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iMOnwA-Yajfn"},"source":["def get_time_per_list(NUM_BINS):\n","    all_paths = natsorted(glob.glob(ROOT + '/data/ocr_paper_COMPREHENSIVE/*/'))\n","    all_path_chunked = np.array_split(all_paths , NUM_BINS)\n","    all_paths = np.array_split(all_paths , NUM_BINS)\n","    time_periods = {}\n","    time_per_list = []\n","    for i , file_chunk in enumerate(all_paths):\n","        time_periods[i] = file_chunk\n","        for j in range(len(time_periods[i])):\n","            time_periods[i][j] = time_periods[i][j].split('/')[-2]\n","        time_periods[i] = str(time_periods[i][0]) + '-' +  str(time_periods[i][-1])\n","        time_per_list.append(time_periods[i])\n","    return (time_per_list, all_path_chunked)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fpPHYlb2db3R"},"source":["(time_per_list, all_path_chunked) = get_time_per_list(NUM_BINS)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wzwD5YgRjwKt","executionInfo":{"status":"ok","timestamp":1634079979679,"user_tz":240,"elapsed":1931,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}},"outputId":"c7e89396-a245-428c-d86c-e31c26fa2bf3"},"source":["all_path_chunked[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['/content/drive/MyDrive/regen_x/data/ocr_paper_COMPREHENSIVE/1776/',\n","       '/content/drive/MyDrive/regen_x/data/ocr_paper_COMPREHENSIVE/1795/',\n","       '/content/drive/MyDrive/regen_x/data/ocr_paper_COMPREHENSIVE/1820/',\n","       '/content/drive/MyDrive/regen_x/data/ocr_paper_COMPREHENSIVE/1824/',\n","       '/content/drive/MyDrive/regen_x/data/ocr_paper_COMPREHENSIVE/1826/',\n","       '/content/drive/MyDrive/regen_x/data/ocr_paper_COMPREHENSIVE/1827/',\n","       '/content/drive/MyDrive/regen_x/data/ocr_paper_COMPREHENSIVE/1828/',\n","       '/content/drive/MyDrive/regen_x/data/ocr_paper_COMPREHENSIVE/1831/',\n","       '/content/drive/MyDrive/regen_x/data/ocr_paper_COMPREHENSIVE/1832/'],\n","      dtype='<U65')"]},"metadata":{},"execution_count":61}]},{"cell_type":"code","metadata":{"id":"JetqIUCinAE8"},"source":["# For gensim we need to split the document into sentences. \n","# Then concatenate all the documents of a given year into one big array with all their sentences. \n","\n","# This functions takes a folder of files and returns one array with \n","# all of the files processed sentences(which themselves are a list of words) as elements \n","# def get_proc_docs(year_path, STARTWORDS, STOPWORDS, max_papers=None, verbose=True):\n","#   file_paths = glob.glob(year_path + \"*.docx\")\n","\n","#   print(\"Number of files: {}\".format(len(file_paths)))\n","#   if len(file_paths) == 0:\n","#     # raise Exception(\"Folder has no files - maybe drive was not mounted?\")\n","#     pass \n","#   ## -- Collecting Papers from Given Year -- ##\n","#   proc_docs = [] \n","\n","#   counter = 1\n","#   length = len(file_paths)\n","#   for f in file_paths:\n","#     # gives it to us in sentences! \n","#     doc = get_docx(f)\n","#     # proc_doc = [word for word in re.findall(r'\\w+', doc.lower()) if ((word in STARTWORDS) and (len(word) > 2) and (word not in STOPWORDS))]\n","\n","#     for sentence in doc:\n","#       # don't think we need to remove stopwords and such if we're training embeddings \n","#       # do lemmatization here as well \n","#       proc_sentence = [word for word in re.findall(r'\\w+', sentence)]\n","#       proc_sentence = do_lemmatizing(proc_sentence) \n","#       proc_docs.append(proc_sentence)  \n","\n","#     if(verbose):\n","#       print(\"\\t{}/{}\".format(counter, length))\n","#     counter += 1\n","\n","#     if max_papers != None:\n","#       if counter == max_papers+1:\n","#         break \n","\n","#   return proc_docs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"79xopbjgug8O"},"source":["# for lemmatization \n","import spacy\n","# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n","nlp = spacy.load('en', disable=['parser', 'ner'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BtM5zNcln_nJ"},"source":["def do_stemming(filtered):\n","\tstemmed = []\n","\tfor f in filtered:\n","\t\tstemmed.append(PorterStemmer().stem(f))\n","\t\t#stemmed.append(LancasterStemmer().stem(f))\n","\t\t#stemmed.append(SnowballStemmer('english').stem(f))\n","\treturn stemmed\n","\n","def do_lemmatizing(filtered):\n","  # convert list to string \n","  spacy_parsed_text = nlp(\" \".join(filtered)) \n","  # Get the lemma for each token in the parsed text \n","  \n","  # I wanted to keep pronouns so not taking lemma if it's a pronoun but if you want to remove pronouns use below commented line \n","  # return \" \".join([token.lemma_ for token in doc])\n","\n","  # return as list of words again \n","  return [token.lemma_ if token.lemma_ != '-PRON-' else token.lower_ for token in spacy_parsed_text]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q2IO2O1znJZW"},"source":["def get_proc_docs(year_path, STARTWORDS, STOPWORDS, max_papers=None, verbose=True, use_porter=False, useStopWords=False):\n","  file_paths = glob.glob(year_path + \"*.docx\")\n","\n","  print(\"Number of files: {}\".format(len(file_paths)))\n","  if len(file_paths) == 0:\n","    # raise Exception(\"Folder has no files - maybe drive was not mounted?\")\n","    pass \n","  ## -- Collecting Papers from Given Year -- ##\n","  proc_docs = [] \n","\n","  counter = 1\n","  length = len(file_paths)\n","  for f in file_paths:\n","    doc = get_docx(f)\n","    \n","    for sentence in doc:\n","      # don't think we need to remove stopwords and such if we're training embeddings \n","      # do lemmatization here as well \n","\n","      proc_sentence = [] \n","      if useStopWords:\n","        proc_sentence = [word for word in re.findall(r'\\w+', sentence) if ((len(word) > 2) and (word not in STOPWORDS))]\n","      else:\n","        proc_sentence = [word for word in re.findall(r'\\w+', sentence)]\n","\n","      if use_porter:\n","        proc_sentence = do_stemming(proc_sentence) \n","      else:\n","        proc_sentence = do_lemmatizing(proc_sentence) \n","      \n","      proc_docs.append(proc_sentence)  \n","\n","    if(verbose):\n","      print(\"\\t{}/{}\".format(counter, length))\n","    counter += 1\n","\n","    if max_papers != None:\n","      if counter == max_papers+1:\n","        break \n","\n","  return proc_docs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FHxMSeusneBl"},"source":["MODEL_PATH = \"/content/drive/MyDrive/Colab Notebooks/NLP - Lab/WordEmbeddings/Models/Gensim_Lemmatized_Without_Stopwords/\"\n","\n","if not os.path.exists(MODEL_PATH):\n","    os.makedirs(MODEL_PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xGLmFO4Bdcz9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634083658200,"user_tz":240,"elapsed":3559551,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}},"outputId":"c68a6feb-c873-41f2-b7e9-fe3c6c4c487f"},"source":["### Train and Save Models for all time periods ##\n","\n","counter = 0 \n","for time_period_paths in all_path_chunked:\n","  cur_time_period = time_per_list[counter] \n","  print(\"Current Time Period: {}\".format(cur_time_period))\n","  all_proc_docs_time_period = [] \n","\n","  for i, year_path in enumerate(time_period_paths):\n","    print(\"{}/{}\".format(i+1, len(time_period_paths)))\n","\n","    proc_doc_cur_year = get_proc_docs(year_path, STARTWORDS, STOPWORDS, max_papers=None, verbose=True, use_porter=False, useStopWords=True) \n","\n","    all_proc_docs_time_period += all_proc_docs_time_period + proc_doc_cur_year \n","\n","  # Train the embeddings! \n","  print(\"Training word embeddings for {}...\".format(cur_time_period))\n","  model = Word2Vec(sentences=all_proc_docs_time_period, min_count=1) \n","\n","  # Sanity check \n","  print(model.wv.most_similar(\"eye\", topn=10))\n","\n","  # Store just the words + their trained embeddings.\n","  word_vectors = model.wv\n","  word_vectors.save(MODEL_PATH + \"{}.wordvectors\".format(cur_time_period))\n","\n","  # Manually create space \n","  del all_proc_docs_time_period\n","  del word_vectors \n","  del model \n","\n","  counter += 1"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Current Time Period: 1776-1832\n","1/9\n","Number of files: 1\n","\t1/1\n","2/9\n","Number of files: 1\n","\t1/1\n","3/9\n","Number of files: 1\n","\t1/1\n","4/9\n","Number of files: 2\n","\t1/2\n","\t2/2\n","5/9\n","Number of files: 1\n","\t1/1\n","6/9\n","Number of files: 1\n","\t1/1\n","7/9\n","Number of files: 2\n","\t1/2\n","\t2/2\n","8/9\n","Number of files: 1\n","\t1/1\n","9/9\n","Number of files: 2\n","\t1/2\n","\t2/2\n","Training word embeddings for 1776-1832...\n","[('person', 0.7005709409713745), ('correspondence', 0.6530425548553467), ('situate', 0.6521275043487549), ('extract', 0.6384673714637756), ('object', 0.6352414488792419), ('squinting', 0.6308776140213013), ('tumour', 0.6193944215774536), ('ponde', 0.6065702438354492), ('partially', 0.602207362651825), ('crecum', 0.6004467010498047)]\n","Current Time Period: 1835-1846\n","1/9\n","Number of files: 2\n","\t1/2\n","\t2/2\n","2/9\n","Number of files: 1\n","\t1/1\n","3/9\n","Number of files: 0\n","4/9\n","Number of files: 1\n","\t1/1\n","5/9\n","Number of files: 1\n","\t1/1\n","6/9\n","Number of files: 1\n","\t1/1\n","7/9\n","Number of files: 1\n","\t1/1\n","8/9\n","Number of files: 1\n","\t1/1\n","9/9\n","Number of files: 2\n","\t1/2\n","\t2/2\n","Training word embeddings for 1835-1846...\n","[('naked', 0.6555576920509338), ('inward', 0.6130425930023193), ('radiation', 0.6018979549407959), ('rolling', 0.5855291485786438), ('iris', 0.5807124376296997), ('affccte', 0.5714719295501709), ('gation', 0.5529786348342896), ('confound', 0.5505732297897339), ('ganglionic', 0.5500450134277344), ('conse11tient', 0.5473181009292603)]\n","Current Time Period: 1847-1859\n","1/8\n","Number of files: 0\n","2/8\n","Number of files: 3\n","\t1/3\n","\t2/3\n","\t3/3\n","3/8\n","Number of files: 1\n","\t1/1\n","4/8\n","Number of files: 1\n","\t1/1\n","5/8\n","Number of files: 1\n","\t1/1\n","6/8\n","Number of files: 2\n","\t1/2\n","\t2/2\n","7/8\n","Number of files: 2\n","\t1/2\n","\t2/2\n","8/8\n","Number of files: 3\n","\t1/3\n","\t2/3\n","\t3/3\n","Training word embeddings for 1847-1859...\n","[('naked', 0.7882230281829834), ('inward', 0.7363426089286804), ('nictitant', 0.7251157164573669), ('mist', 0.7181519865989685), ('termin', 0.7160534262657166), ('lnlffere', 0.715760350227356), ('ball', 0.712509036064148), ('405', 0.7071590423583984), ('prepare', 0.7021614909172058), ('vas', 0.7011250257492065)]\n","Current Time Period: 1860-1872\n","1/8\n","Number of files: 2\n","\t1/2\n","\t2/2\n","2/8\n","Number of files: 1\n","\t1/1\n","3/8\n","Number of files: 2\n","\t1/2\n","\t2/2\n","4/8\n","Number of files: 1\n","\t1/1\n","5/8\n","Number of files: 3\n","\t1/3\n","\t2/3\n","\t3/3\n","6/8\n","Number of files: 1\n","\t1/1\n","7/8\n","Number of files: 1\n","\t1/1\n","8/8\n","Number of files: 0\n","Training word embeddings for 1860-1872...\n","[('assert', 0.6227784752845764), ('walked', 0.5902915000915527), ('smgeon', 0.5391013026237488), ('hotter', 0.5377715229988098), ('trary', 0.5120881795883179), ('suellen', 0.5103526711463928), ('knits', 0.5012633800506592), ('depresse', 0.49995678663253784), ('ear', 0.4943990707397461), ('nostril', 0.49078303575515747)]\n","Current Time Period: 1874-1884\n","1/8\n","Number of files: 2\n","\t1/2\n","\t2/2\n","2/8\n","Number of files: 1\n","\t1/1\n","3/8\n","Number of files: 1\n","\t1/1\n","4/8\n","Number of files: 2\n","\t1/2\n","\t2/2\n","5/8\n","Number of files: 1\n","\t1/1\n","6/8\n","Number of files: 1\n","\t1/1\n","7/8\n","Number of files: 4\n","\t1/4\n","\t2/4\n","\t3/4\n","\t4/4\n","8/8\n","Number of files: 3\n","\t1/3\n","\t2/3\n","\t3/3\n","Training word embeddings for 1874-1884...\n","[('distinguish', 0.7954697012901306), ('difference', 0.7833780646324158), ('inflamma', 0.7355793714523315), ('signal', 0.7222335338592529), ('chest', 0.7149831056594849), ('sore', 0.6866342425346375), ('inflammation', 0.6861382722854614), ('throat', 0.6801146268844604), ('vowel', 0.6643872857093811), ('colour', 0.6561815738677979)]\n","Current Time Period: 1885-1894\n","1/8\n","Number of files: 2\n","\t1/2\n","\t2/2\n","2/8\n","Number of files: 1\n","\t1/1\n","3/8\n","Number of files: 2\n","\t1/2\n","\t2/2\n","4/8\n","Number of files: 1\n","\t1/1\n","5/8\n","Number of files: 1\n","\t1/1\n","6/8\n","Number of files: 2\n","\t1/2\n","\t2/2\n","7/8\n","Number of files: 1\n","\t1/1\n","8/8\n","Number of files: 1\n","\t1/1\n","Training word embeddings for 1885-1894...\n","[('fiftieth', 0.7223973274230957), ('excite', 0.692867636680603), ('traumatic', 0.6691468358039856), ('rub', 0.659831166267395), ('left', 0.6435489654541016), ('equal', 0.6402757167816162), ('enucleation', 0.6329034566879272), ('shut', 0.6326925754547119), ('sacrifice', 0.6295336484909058), ('irritation', 0.6264131665229797)]\n","Current Time Period: 1895-1902\n","1/8\n","Number of files: 2\n","\t1/2\n","\t2/2\n","2/8\n","Number of files: 3\n","\t1/3\n","\t2/3\n","\t3/3\n","3/8\n","Number of files: 4\n","\t1/4\n","\t2/4\n","\t3/4\n","\t4/4\n","4/8\n","Number of files: 3\n","\t1/3\n","\t2/3\n","\t3/3\n","5/8\n","Number of files: 2\n","\t1/2\n","\t2/2\n","6/8\n","Number of files: 1\n","\t1/1\n","7/8\n","Number of files: 2\n","\t1/2\n","\t2/2\n","8/8\n","Number of files: 3\n","\t1/3\n","\t2/3\n","\t3/3\n","Training word embeddings for 1895-1902...\n","[('palpebral', 0.6585674285888672), ('shut', 0.6511270403862), ('eyelid', 0.6186392307281494), ('eai', 0.6091023683547974), ('protrusion', 0.6061821579933167), ('ork', 0.601423978805542), ('aperture', 0.6001356244087219), ('hair', 0.5616977214813232), ('microsoope', 0.5594321489334106), ('symptoms', 0.5562670230865479)]\n","Current Time Period: 1903-1910\n","1/8\n","Number of files: 1\n","\t1/1\n","2/8\n","Number of files: 1\n","\t1/1\n","3/8\n","Number of files: 3\n","\t1/3\n","\t2/3\n","\t3/3\n","4/8\n","Number of files: 5\n","\t1/5\n","\t2/5\n","\t3/5\n","\t4/5\n","\t5/5\n","5/8\n","Number of files: 5\n","\t1/5\n","\t2/5\n","\t3/5\n","\t4/5\n","\t5/5\n","6/8\n","Number of files: 11\n","\t1/11\n","\t2/11\n","\t3/11\n","\t4/11\n","\t5/11\n","\t6/11\n","\t7/11\n","\t8/11\n","\t9/11\n","\t10/11\n","\t11/11\n","7/8\n","Number of files: 5\n","\t1/5\n","\t2/5\n","\t3/5\n","\t4/5\n","\t5/5\n","8/8\n","Number of files: 6\n","\t1/6\n","\t2/6\n","\t3/6\n","\t4/6\n","\t5/6\n","\t6/6\n","Training word embeddings for 1903-1910...\n","[('deaf', 0.4947899580001831), ('glass', 0.42747846245765686), ('cornea', 0.4178631603717804), ('treme', 0.39422905445098877), ('esmarch', 0.3879272937774658), ('emme', 0.3878888785839081), ('diplopia', 0.38651517033576965), ('eaoh', 0.3726939558982849), ('circuit', 0.3696495294570923), ('binocular', 0.3676823675632477)]\n","Current Time Period: 1911-1918\n","1/8\n","Number of files: 5\n","\t1/5\n","\t2/5\n","\t3/5\n","\t4/5\n","\t5/5\n","2/8\n","Number of files: 6\n","\t1/6\n","\t2/6\n","\t3/6\n","\t4/6\n","\t5/6\n","\t6/6\n","3/8\n","Number of files: 10\n","\t1/10\n","\t2/10\n","\t3/10\n","\t4/10\n","\t5/10\n","\t6/10\n","\t7/10\n","\t8/10\n","\t9/10\n","\t10/10\n","4/8\n","Number of files: 7\n","\t1/7\n","\t2/7\n","\t3/7\n","\t4/7\n","\t5/7\n","\t6/7\n","\t7/7\n","5/8\n","Number of files: 8\n","\t1/8\n","\t2/8\n","\t3/8\n","\t4/8\n","\t5/8\n","\t6/8\n","\t7/8\n","\t8/8\n","6/8\n","Number of files: 5\n","\t1/5\n","\t2/5\n","\t3/5\n","\t4/5\n","\t5/5\n","7/8\n","Number of files: 2\n","\t1/2\n","\t2/2\n","8/8\n","Number of files: 5\n","\t1/5\n","\t2/5\n","\t3/5\n","\t4/5\n","\t5/5\n","Training word embeddings for 1911-1918...\n","[('tracture', 0.47585147619247437), ('intraocular', 0.46522367000579834), ('iliacus', 0.45876437425613403), ('rectus', 0.45206278562545776), ('oup', 0.4456920623779297), ('cumulative', 0.44046422839164734), ('quahty', 0.42806917428970337), ('head', 0.42385244369506836), ('phylogenetically', 0.4088822603225708), ('isola', 0.4058704674243927)]\n","Current Time Period: 1919-1926\n","1/8\n","Number of files: 11\n","\t1/11\n","\t2/11\n","\t3/11\n","\t4/11\n","\t5/11\n","\t6/11\n","\t7/11\n","\t8/11\n","\t9/11\n","\t10/11\n","\t11/11\n","2/8\n","Number of files: 3\n","\t1/3\n","\t2/3\n","\t3/3\n","3/8\n","Number of files: 6\n","\t1/6\n","\t2/6\n","\t3/6\n","\t4/6\n","\t5/6\n","\t6/6\n","4/8\n","Number of files: 5\n","\t1/5\n","\t2/5\n","\t3/5\n","\t4/5\n","\t5/5\n","5/8\n","Number of files: 6\n","\t1/6\n","\t2/6\n","\t3/6\n","\t4/6\n","\t5/6\n","\t6/6\n","6/8\n","Number of files: 4\n","\t1/4\n","\t2/4\n","\t3/4\n","\t4/4\n","7/8\n","Number of files: 6\n","\t1/6\n","\t2/6\n","\t3/6\n","\t4/6\n","\t5/6\n","\t6/6\n","8/8\n","Number of files: 2\n","\t1/2\n","\t2/2\n","Training word embeddings for 1919-1926...\n","[('subdivision', 0.446895569562912), ('wllen', 0.43048742413520813), ('incoordine', 0.4298245310783386), ('vision', 0.42034921050071716), ('expression', 0.420007586479187), ('duplex', 0.41973817348480225), ('ool01', 0.4083458483219147), ('bathing', 0.3933916985988617), ('functionless', 0.38965511322021484), ('rouse', 0.38928234577178955)]\n","Current Time Period: 1927-1935\n","1/8\n","Number of files: 4\n","\t1/4\n","\t2/4\n","\t3/4\n","\t4/4\n","2/8\n","Number of files: 2\n","\t1/2\n","\t2/2\n","3/8\n","Number of files: 4\n","\t1/4\n","\t2/4\n","\t3/4\n","\t4/4\n","4/8\n","Number of files: 6\n","\t1/6\n","\t2/6\n","\t3/6\n","\t4/6\n","\t5/6\n","\t6/6\n","5/8\n","Number of files: 6\n","\t1/6\n","\t2/6\n","\t3/6\n","\t4/6\n","\t5/6\n","\t6/6\n","6/8\n","Number of files: 4\n","\t1/4\n","\t2/4\n","\t3/4\n","\t4/4\n","7/8\n","Number of files: 13\n","\t1/13\n","\t2/13\n","\t3/13\n","\t4/13\n","\t5/13\n","\t6/13\n","\t7/13\n","\t8/13\n","\t9/13\n","\t10/13\n","\t11/13\n","\t12/13\n","\t13/13\n","8/8\n","Number of files: 13\n","\t1/13\n","\t2/13\n","\t3/13\n","\t4/13\n","\t5/13\n","\t6/13\n","\t7/13\n","\t8/13\n","\t9/13\n","\t10/13\n","\t11/13\n","\t12/13\n","\t13/13\n","Training word embeddings for 1927-1935...\n","[('devi', 0.4552730619907379), ('vision', 0.44894376397132874), ('bump', 0.4489012062549591), ('blind', 0.44207215309143066), ('color', 0.4306878447532654), ('hemianopsia', 0.4255298376083374), ('slowly', 0.4172348380088806), ('ness', 0.41605740785598755), ('toma', 0.41487133502960205), ('bitemporal', 0.4084692597389221)]\n","Current Time Period: 1936-1943\n","1/8\n","Number of files: 12\n","\t1/12\n","\t2/12\n","\t3/12\n","\t4/12\n","\t5/12\n","\t6/12\n","\t7/12\n","\t8/12\n","\t9/12\n","\t10/12\n","\t11/12\n","\t12/12\n","2/8\n","Number of files: 7\n","\t1/7\n","\t2/7\n","\t3/7\n","\t4/7\n","\t5/7\n","\t6/7\n","\t7/7\n","3/8\n","Number of files: 6\n","\t1/6\n","\t2/6\n","\t3/6\n","\t4/6\n","\t5/6\n","\t6/6\n","4/8\n","Number of files: 8\n","\t1/8\n","\t2/8\n","\t3/8\n","\t4/8\n","\t5/8\n","\t6/8\n","\t7/8\n","\t8/8\n","5/8\n","Number of files: 5\n","\t1/5\n","\t2/5\n","\t3/5\n","\t4/5\n","\t5/5\n","6/8\n","Number of files: 12\n","\t1/12\n","\t2/12\n","\t3/12\n","\t4/12\n","\t5/12\n","\t6/12\n","\t7/12\n","\t8/12\n","\t9/12\n","\t10/12\n","\t11/12\n","\t12/12\n","7/8\n","Number of files: 7\n","\t1/7\n","\t2/7\n","\t3/7\n","\t4/7\n","\t5/7\n","\t6/7\n","\t7/7\n","8/8\n","Number of files: 8\n","\t1/8\n","\t2/8\n","\t3/8\n","\t4/8\n","\t5/8\n","\t6/8\n","\t7/8\n","\t8/8\n","Training word embeddings for 1936-1943...\n","[('peripherally', 0.4298233389854431), ('object', 0.40160244703292847), ('lens', 0.3945367932319641), ('mirror', 0.38018208742141724), ('enlarge', 0.37924203276634216), ('dizzy', 0.37140733003616333), ('toto', 0.3631007671356201), ('perimeter', 0.35685116052627563), ('meridian', 0.3539363145828247), ('astigmatism', 0.35079070925712585)]\n","Current Time Period: 1944-1974\n","1/8\n","Number of files: 8\n","\t1/8\n","\t2/8\n","\t3/8\n","\t4/8\n","\t5/8\n","\t6/8\n","\t7/8\n","\t8/8\n","2/8\n","Number of files: 0\n","3/8\n","Number of files: 2\n","\t1/2\n","\t2/2\n","4/8\n","Number of files: 1\n","\t1/1\n","5/8\n","Number of files: 1\n","\t1/1\n","6/8\n","Number of files: 1\n","\t1/1\n","7/8\n","Number of files: 1\n","\t1/1\n","8/8\n","Number of files: 1\n","\t1/1\n","Training word embeddings for 1944-1974...\n","[('horizontal', 0.41315174102783203), ('rotatory', 0.377031147480011), ('pharynx', 0.37633809447288513), ('sardonicus', 0.3720732629299164), ('lago', 0.34577763080596924), ('brachial', 0.33260011672973633), ('rectus', 0.3308641314506531), ('mocturma', 0.330726683139801), ('oblique', 0.3276369869709015), ('larynx', 0.32736635208129883)]\n","Current Time Period: 1977-1988\n","1/8\n","Number of files: 1\n","\t1/1\n","2/8\n","Number of files: 2\n","\t1/2\n","\t2/2\n","3/8\n","Number of files: 1\n","\t1/1\n","4/8\n","Number of files: 1\n","\t1/1\n","5/8\n","Number of files: 1\n","\t1/1\n","6/8\n","Number of files: 1\n","\t1/1\n","7/8\n","Number of files: 1\n","\t1/1\n","8/8\n","Number of files: 1\n","\t1/1\n","Training word embeddings for 1977-1988...\n","[('autopsy', 0.7631397247314453), ('unaided', 0.7291663885116577), ('gross', 0.7285476922988892), ('continuity', 0.7111164331436157), ('serine', 0.6935864686965942), ('proximal', 0.6924853324890137), ('frequently', 0.6899751424789429), ('oufulr', 0.6831641793251038), ('special', 0.6781850457191467), ('aligned', 0.6773194074630737)]\n","Current Time Period: 1989-1996\n","1/8\n","Number of files: 4\n","\t1/4\n","\t2/4\n","\t3/4\n","\t4/4\n","2/8\n","Number of files: 12\n","\t1/12\n","\t2/12\n","\t3/12\n","\t4/12\n","\t5/12\n","\t6/12\n","\t7/12\n","\t8/12\n","\t9/12\n","\t10/12\n","\t11/12\n","\t12/12\n","3/8\n","Number of files: 7\n","\t1/7\n","\t2/7\n","\t3/7\n","\t4/7\n","\t5/7\n","\t6/7\n","\t7/7\n","4/8\n","Number of files: 2\n","\t1/2\n","\t2/2\n","5/8\n","Number of files: 7\n","\t1/7\n","\t2/7\n","\t3/7\n","\t4/7\n","\t5/7\n","\t6/7\n","\t7/7\n","6/8\n","Number of files: 7\n","\t1/7\n","\t2/7\n","\t3/7\n","\t4/7\n","\t5/7\n","\t6/7\n","\t7/7\n","7/8\n","Number of files: 20\n","\t1/20\n","\t2/20\n","\t3/20\n","\t4/20\n","\t5/20\n","\t6/20\n","\t7/20\n","\t8/20\n","\t9/20\n","\t10/20\n","\t11/20\n","\t12/20\n","\t13/20\n","\t14/20\n","\t15/20\n","\t16/20\n","\t17/20\n","\t18/20\n","\t19/20\n","\t20/20\n","8/8\n","Number of files: 10\n","\t1/10\n","\t2/10\n","\t3/10\n","\t4/10\n","\t5/10\n","\t6/10\n","\t7/10\n","\t8/10\n","\t9/10\n","\t10/10\n","Training word embeddings for 1989-1996...\n","[('optic', 0.46582767367362976), ('tract', 0.42622023820877075), ('chiasma', 0.40608543157577515), ('head', 0.4001142382621765), ('transection', 0.38756299018859863), ('mount', 0.3796306848526001), ('labeling', 0.373996376991272), ('goldschleger', 0.36650216579437256), ('entrance', 0.36545079946517944), ('hemicord', 0.36041104793548584)]\n","Current Time Period: 1997-2015\n","1/8\n","Number of files: 8\n","\t1/8\n","\t2/8\n","\t3/8\n","\t4/8\n","\t5/8\n","\t6/8\n","\t7/8\n","\t8/8\n","2/8\n","Number of files: 10\n","\t1/10\n","\t2/10\n","\t3/10\n","\t4/10\n","\t5/10\n","\t6/10\n","\t7/10\n","\t8/10\n","\t9/10\n","\t10/10\n","3/8\n","Number of files: 15\n","\t1/15\n","\t2/15\n","\t3/15\n","\t4/15\n","\t5/15\n","\t6/15\n","\t7/15\n","\t8/15\n","\t9/15\n","\t10/15\n","\t11/15\n","\t12/15\n","\t13/15\n","\t14/15\n","\t15/15\n","4/8\n","Number of files: 9\n","\t1/9\n","\t2/9\n","\t3/9\n","\t4/9\n","\t5/9\n","\t6/9\n","\t7/9\n","\t8/9\n","\t9/9\n","5/8\n","Number of files: 59\n","\t1/59\n","\t2/59\n","\t3/59\n","\t4/59\n","\t5/59\n","\t6/59\n","\t7/59\n","\t8/59\n","\t9/59\n","\t10/59\n","\t11/59\n","\t12/59\n","\t13/59\n","\t14/59\n","\t15/59\n","\t16/59\n","\t17/59\n","\t18/59\n","\t19/59\n","\t20/59\n","\t21/59\n","\t22/59\n","\t23/59\n","\t24/59\n","\t25/59\n","\t26/59\n","\t27/59\n","\t28/59\n","\t29/59\n","\t30/59\n","\t31/59\n","\t32/59\n","\t33/59\n","\t34/59\n","\t35/59\n","\t36/59\n","\t37/59\n","\t38/59\n","\t39/59\n","\t40/59\n","\t41/59\n","\t42/59\n","\t43/59\n","\t44/59\n","\t45/59\n","\t46/59\n","\t47/59\n","\t48/59\n","\t49/59\n","\t50/59\n","\t51/59\n","\t52/59\n","\t53/59\n","\t54/59\n","\t55/59\n","\t56/59\n","\t57/59\n","\t58/59\n","\t59/59\n","6/8\n","Number of files: 17\n","\t1/17\n","\t2/17\n","\t3/17\n","\t4/17\n","\t5/17\n","\t6/17\n","\t7/17\n","\t8/17\n","\t9/17\n","\t10/17\n","\t11/17\n","\t12/17\n","\t13/17\n","\t14/17\n","\t15/17\n","\t16/17\n","\t17/17\n","7/8\n","Number of files: 98\n","\t1/98\n","\t2/98\n","\t3/98\n","\t4/98\n","\t5/98\n","\t6/98\n","\t7/98\n","\t8/98\n","\t9/98\n","\t10/98\n","\t11/98\n","\t12/98\n","\t13/98\n","\t14/98\n","\t15/98\n","\t16/98\n","\t17/98\n","\t18/98\n","\t19/98\n","\t20/98\n","\t21/98\n","\t22/98\n","\t23/98\n","\t24/98\n","\t25/98\n","\t26/98\n","\t27/98\n","\t28/98\n","\t29/98\n","\t30/98\n","\t31/98\n","\t32/98\n","\t33/98\n","\t34/98\n","\t35/98\n","\t36/98\n","\t37/98\n","\t38/98\n","\t39/98\n","\t40/98\n","\t41/98\n","\t42/98\n","\t43/98\n","\t44/98\n","\t45/98\n","\t46/98\n","\t47/98\n","\t48/98\n","\t49/98\n","\t50/98\n","\t51/98\n","\t52/98\n","\t53/98\n","\t54/98\n","\t55/98\n","\t56/98\n","\t57/98\n","\t58/98\n","\t59/98\n","\t60/98\n","\t61/98\n","\t62/98\n","\t63/98\n","\t64/98\n","\t65/98\n","\t66/98\n","\t67/98\n","\t68/98\n","\t69/98\n","\t70/98\n","\t71/98\n","\t72/98\n","\t73/98\n","\t74/98\n","\t75/98\n","\t76/98\n","\t77/98\n","\t78/98\n","\t79/98\n","\t80/98\n","\t81/98\n","\t82/98\n","\t83/98\n","\t84/98\n","\t85/98\n","\t86/98\n","\t87/98\n","\t88/98\n","\t89/98\n","\t90/98\n","\t91/98\n","\t92/98\n","\t93/98\n","\t94/98\n","\t95/98\n","\t96/98\n","\t97/98\n","\t98/98\n","8/8\n","Number of files: 54\n","\t1/54\n","\t2/54\n","\t3/54\n","\t4/54\n","\t5/54\n","\t6/54\n","\t7/54\n","\t8/54\n","\t9/54\n","\t10/54\n","\t11/54\n","\t12/54\n","\t13/54\n","\t14/54\n","\t15/54\n","\t16/54\n","\t17/54\n","\t18/54\n","\t19/54\n","\t20/54\n","\t21/54\n","\t22/54\n","\t23/54\n","\t24/54\n","\t25/54\n","\t26/54\n","\t27/54\n","\t28/54\n","\t29/54\n","\t30/54\n","\t31/54\n","\t32/54\n","\t33/54\n","\t34/54\n","\t35/54\n","\t36/54\n","\t37/54\n","\t38/54\n","\t39/54\n","\t40/54\n","\t41/54\n","\t42/54\n","\t43/54\n","\t44/54\n","\t45/54\n","\t46/54\n","\t47/54\n","\t48/54\n","\t49/54\n","\t50/54\n","\t51/54\n","\t52/54\n","\t53/54\n","\t54/54\n","Training word embeddings for 1997-2015...\n","[('retina', 0.4635256826877594), ('mont', 0.4605802893638611), ('lens', 0.4514198899269104), ('turbinate', 0.4427381753921509), ('omega', 0.4316984713077545), ('position', 0.40108734369277954), ('etoh', 0.39377373456954956), ('curved', 0.3919978737831116), ('embryos', 0.3862312138080597), ('limb', 0.38513505458831787)]\n"]}]},{"cell_type":"code","metadata":{"id":"uIPvngFbowMh"},"source":["# Exceution of above function took: 59 mins 19 seconds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zwK59B1x0DvC"},"source":[],"execution_count":null,"outputs":[]}]}