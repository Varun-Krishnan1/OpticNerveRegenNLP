{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"provenance":[]}},"cells":[{"cell_type":"markdown","source":["* This file generates a comparison dataframe (see last cell) to compare different combinations of stemming, stopwords, and type of algorithm used (gensim vs glove):\n","  * stemming + stopwords + gensim\n","  * stemming + no stopwords + gensim\n","  * stemming + stopwords + glove\n","  * stemming + no stopwords + glove \n","  * lemmatizing + stopwords + gensim\n","  * lemmatizing + no stopwords + gensim\n","  * lemmatizing + stopwords + glove\n","  * lemmatizing + no stopwords + glove \n","* Gensim uses CBOW (neural network based) for training vs glove that uses word co-association matrix (no neural network)\n","* The models were empirically compared with different years and and were compared (using most_similar) with top 20 words close to an empirically chosen word \n","  * See the word embedding presentation linked here ____ for different years and words they were compared against \n","* **Conclusion: Empirically determined that Gensim model with lemmatization and stopwords included is the best approach**"],"metadata":{"id":"yGsic-OaFFYs"}},{"cell_type":"markdown","metadata":{"id":"Z-acCjLbFeJt"},"source":["# Training GloVe model on neuroscience papers"]},{"cell_type":"markdown","source":["# Conclusion\n","\n","Gensim model with lemmatization and stopwords included is the best approach"],"metadata":{"id":"k0mNWxmYQweF"}},{"cell_type":"markdown","source":["# Testing"],"metadata":{"id":"h76urikqRCXt"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j_HTqeOlLwkp","executionInfo":{"status":"ok","timestamp":1635435382196,"user_tz":240,"elapsed":7991,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}},"outputId":"1628d208-5951-490d-aa41-ab8f15ecab30"},"source":["!pip install python-docx"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting python-docx\n","  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n","\u001b[K     |████████████████████████████████| 5.6 MB 8.5 MB/s \n","\u001b[?25hRequirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from python-docx) (4.2.6)\n","Building wheels for collected packages: python-docx\n","  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184508 sha256=8d2e573bc0b3b732d5cc50e9803869659ef042203d1fa202d9cd4eb16c3c366b\n","  Stored in directory: /root/.cache/pip/wheels/f6/6f/b9/d798122a8b55b74ad30b5f52b01482169b445fbb84a11797a6\n","Successfully built python-docx\n","Installing collected packages: python-docx\n","Successfully installed python-docx-0.8.11\n"]}]},{"cell_type":"code","metadata":{"id":"70HN_RS6T8OD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635435385673,"user_tz":240,"elapsed":3481,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}},"outputId":"160fd082-7228-46b9-ca89-56c6b41796b4"},"source":["!pip install glove_python-binary"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting glove_python-binary\n","  Downloading glove_python_binary-0.2.0-cp37-cp37m-manylinux1_x86_64.whl (948 kB)\n","\u001b[?25l\r\u001b[K     |▍                               | 10 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |▊                               | 20 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |█                               | 30 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 40 kB 23.4 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 51 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██                              | 61 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 71 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 81 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |███                             | 92 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 102 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 112 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 122 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 133 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 143 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 153 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 163 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 174 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 184 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 194 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████                         | 204 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 215 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 225 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████                        | 235 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 245 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 256 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 266 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 276 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 286 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 296 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 307 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 317 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 327 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 337 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 348 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 358 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 368 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 378 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 389 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 399 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 409 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 419 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 430 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 440 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 450 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 460 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 471 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 481 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 491 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 501 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 512 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 522 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 532 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 542 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 552 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 563 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 573 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 583 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 593 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 604 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 614 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 624 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 634 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 645 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 655 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 665 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 675 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 686 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 696 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 706 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 716 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 727 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 737 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 747 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 757 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 768 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 778 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 788 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 798 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 808 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 819 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 829 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 839 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 849 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 860 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 870 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 880 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 890 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 901 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 911 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 921 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 931 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 942 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 948 kB 9.0 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from glove_python-binary) (1.19.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from glove_python-binary) (1.4.1)\n","Installing collected packages: glove-python-binary\n","Successfully installed glove-python-binary-0.2.0\n"]}]},{"cell_type":"code","metadata":{"id":"jqQNyuxMFeJw"},"source":["import numpy as np\n","\n","import matplotlib.pyplot as plt\n","\n","from sklearn.decomposition import PCA\n","\n","from gensim.test.utils import datapath, get_tmpfile\n","from gensim.models import KeyedVectors\n","from gensim.scripts.glove2word2vec import glove2word2vec"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CYDKvAMYLy4s","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635435388478,"user_tz":240,"elapsed":1286,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}},"outputId":"3d4132e7-1261-42f6-c987-31f11aa8061b"},"source":["from docx import Document\n","import nltk\n","nltk.download('punkt')\n","import re\n","from nltk import sent_tokenize\n","import pandas as pd\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","import pickle\n","import numpy as np\n","import glob"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"code","metadata":{"id":"BDPlGB-qwa6Q"},"source":["from nltk.stem.porter import PorterStemmer\n","from nltk.stem.lancaster import LancasterStemmer\n","from nltk.stem import SnowballStemmer \n","from nltk.stem import WordNetLemmatizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m7tEPwlGaLng","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635435388954,"user_tz":240,"elapsed":479,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}},"outputId":"0727d3be-d2a8-4440-c4d5-f8c191036cd7"},"source":["import nltk \n","nltk.download('wordnet')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"5_Ow_ShJUIfb"},"source":["from glove import Corpus, Glove"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qJqALNlWL07y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684083422400,"user_tz":240,"elapsed":1651,"user":{"displayName":"Varun Krishnan","userId":"10781844920353862932"}},"outputId":"40ad6184-5732-4e28-ba4e-60f9feead2c0"},"source":["!git clone 'https://github.com/igorbrigadir/stopwords.git'"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'stopwords'...\n","remote: Enumerating objects: 149, done.\u001b[K\n","remote: Total 149 (delta 0), reused 0 (delta 0), pack-reused 149\u001b[K\n","Receiving objects: 100% (149/149), 85.27 KiB | 498.00 KiB/s, done.\n","Resolving deltas: 100% (52/52), done.\n"]}]},{"cell_type":"code","metadata":{"id":"JhyPjMvkL3fb","executionInfo":{"status":"ok","timestamp":1684083433143,"user_tz":240,"elapsed":2,"user":{"displayName":"Varun Krishnan","userId":"10781844920353862932"}}},"source":["alir3z4_data = '/content/stopwords/en/alir3z4.txt'\n","\n","more_stops = pd.read_csv('/content/stopwords/en/alir3z4.txt')\n","new_stops = list(more_stops[\"'ll\"])"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"2QIGTVVjMGw3"},"source":["DOMAIN_STOPS = {'pubmed', 'et', 'al', 'page'}\n","STOPWORDS =  set(stopwords.words('english') + stopwords.words('german') +  stopwords.words('dutch') + stopwords.words('french') +  stopwords.words('spanish')  + new_stops) | DOMAIN_STOPS\n","STOPWORDS = set(STOPWORDS)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XAIUEHTcyP4K"},"source":["ROOT = \"/content/drive/MyDrive/regen_x\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zh1-4X01c6GF"},"source":["# for lemmatization \n","import spacy\n","# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n","nlp = spacy.load('en', disable=['parser', 'ner'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CohG2_FkMKh5"},"source":["def get_docx(file_path):\n","    doc = []\n","    for para in Document(file_path).paragraphs:\n","        if para.text == \"\":\n","            continue\n","        doc += (sent_tokenize(para.text.lower()))\n","    return doc\n","\n","\n","# This functions takes a folder of files and returns one array with \n","# all of the files processed sentences(which themselves are a list of words) as elements \n","# def get_proc_docs(training_paper_year, STARTWORDS, STOPWORDS, max_papers=None, verbose=True, use_porter=False, useStopWords=False):\n","#   global_path = \"/content/drive/MyDrive/regen_x/data/ocr_paper_COMPREHENSIVE/\"\n","#   folder_path = global_path + \"{}/\".format(training_paper_year)\n","#   print(folder_path) \n","#   file_paths = glob.glob(folder_path + \"*.docx\")\n","\n","#   print(\"Number of files: {}\".format(len(file_paths)))\n","#   if len(file_paths) == 0:\n","#     raise Exception(\"Folder has no files - maybe drive was not mounted?\")\n","#   ## -- Collecting Papers from Given Year -- ##\n","#   proc_docs = [] \n","\n","#   counter = 1\n","#   length = len(file_paths)\n","#   for f in file_paths:\n","#     doc = get_docx(f)\n","    \n","#     for sentence in doc:\n","#       # don't think we need to remove stopwords and such if we're training embeddings \n","#       # do lemmatization here as well \n","\n","#       proc_sentence = [] \n","#       if useStopWords:\n","#         proc_sentence = [word for word in re.findall(r'\\w+', sentence)]\n","#       else:\n","#         proc_sentence = [word for word in re.findall(r'\\w+', sentence)]\n","\n","#       if use_porter:\n","#         proc_sentence = do_stemming(proc_sentence) \n","#       else:\n","#         proc_sentence = do_lemmatizing(proc_sentence) \n","\n","#       if useStopWords and use_porter:\n","#         proc_sentence = [word for word in proc_sentence if word not in STEMMED_STOPWORDS]\n","#       elif useStopWords and not use_porter:\n","#         proc_sentence = [word for word in proc_sentence if word not in LEMMATIZED_STOPWORDS]\n","\n","#       proc_docs.append(proc_sentence)  \n","\n","#     if(verbose):\n","#       print(\"\\t{}/{}\".format(counter, length))\n","#     counter += 1\n","\n","#     if max_papers != None:\n","#       if counter == max_papers+1:\n","#         break \n","\n","#   return proc_docs\n","\n","def get_proc_docs(training_paper_year, STARTWORDS, STOPWORDS, max_papers=None, verbose=True, use_porter=False, useStopWords=False):\n","  global_path = \"/content/drive/MyDrive/regen_x/data/ocr_paper_COMPREHENSIVE/\"\n","  folder_path = global_path + \"{}/\".format(training_paper_year)\n","  print(folder_path) \n","  file_paths = glob.glob(folder_path + \"*.docx\")\n","\n","  print(\"Number of files: {}\".format(len(file_paths)))\n","  if len(file_paths) == 0:\n","    raise Exception(\"Folder has no files - maybe drive was not mounted?\")\n","  ## -- Collecting Papers from Given Year -- ##\n","  proc_docs = [] \n","\n","  counter = 1\n","  length = len(file_paths)\n","  for f in file_paths:\n","    doc = get_docx(f)\n","    \n","    for sentence in doc:\n","      proc_sentence = [] \n","      if useStopWords:\n","        proc_sentence = [word for word in re.findall(r'\\w+', sentence) if ((len(word) > 2) and (word not in STOPWORDS))]\n","      else:\n","        proc_sentence = [word for word in re.findall(r'\\w+', sentence)]\n","\n","      if use_porter:\n","        proc_sentence = do_stemming(proc_sentence) \n","      else:\n","        proc_sentence = do_lemmatizing(proc_sentence) \n","\n","      proc_docs.append(proc_sentence)  \n","\n","    if(verbose):\n","      print(\"\\t{}/{}\".format(counter, length))\n","    counter += 1\n","\n","    if max_papers != None:\n","      if counter == max_papers+1:\n","        break \n","\n","  return proc_docs\n","\n","def do_stemming(filtered):\n","\tstemmed = []\n","\tfor f in filtered:\n","\t\tstemmed.append(PorterStemmer().stem(f))\n","\t\t#stemmed.append(LancasterStemmer().stem(f))\n","\t\t#stemmed.append(SnowballStemmer('english').stem(f))\n","\treturn stemmed\n","\n","def do_lemmatizing(filtered):\n","  # convert list to string \n","  spacy_parsed_text = nlp(\" \".join(filtered)) \n","  # Get the lemma for each token in the parsed text \n","  \n","  # I wanted to keep pronouns so not taking lemma if it's a pronoun but if you want to remove pronouns use below commented line \n","  # return \" \".join([token.lemma_ for token in doc])\n","\n","  # return as list of words again \n","  return [token.lemma_ if token.lemma_ != '-PRON-' else token.lower_ for token in spacy_parsed_text]\n"," \n","\n","def get_start_stop():\n","    domain_stops = {'pubmed', 'et', 'al', 'page'}\n","    with open('/content/stopwords/en/alir3z4.txt', 'r') as fn:\n","        new_stops = [line.strip() for line in fn.readlines()]\n","    STOPWORDS =  set(stopwords.words('english') + stopwords.words('german') +  stopwords.words('dutch') + stopwords.words('french') +  stopwords.words('spanish')  + new_stops) | domain_stops\n","\n","    fn = glob.glob(ROOT + '/data/start-words/*')\n","    ALL_STARTS = [pickle.load(open(f , 'rb')) for f in fn]\n","    STARTWORDS = {}\n","    for f in ALL_STARTS:\n","      STARTWORDS.update(f)\n","    STARTWORDS = set(STARTWORDS.keys())\n","\n","    assert(type(STOPWORDS)==set and type(STARTWORDS)==set)\n","    return (STARTWORDS, STOPWORDS)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZoJbCIdJVwBq"},"source":["STARTWORDS, STOPWORDS = get_start_stop()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wts9qXiFgewW"},"source":["STEMMED_STOPWORDS = do_stemming(STOPWORDS) \n","LEMMATIZED_STOPWORDS = do_lemmatizing(STOPWORDS)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GNrqnEJdUzcG"},"source":["# Glove Model"]},{"cell_type":"code","metadata":{"id":"VpenrX1dtUZq"},"source":["def train_glove(proc_docs):\n","  #Creating a corpus object\n","  corpus = Corpus() \n","\n","  #Training the corpus to generate the co occurence matrix which is used in GloVe\n","  corpus.fit(proc_docs, window=10)\n","\n","  glove = Glove(no_components=5, learning_rate=0.05) \n","  glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)\n","  glove.add_dictionary(corpus.dictionary)\n","  # glove.save('glove.model')\n","\n","  return glove "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WjKLHDMRNXSa"},"source":["def get_glove_model(year, STARTWORDS, STOPWORDS, max_papers=None):\n","  proc_docs = get_proc_docs(year, STARTWORDS, STOPWORDS, max_papers)\n","  return train_glove(proc_docs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RQ2G6RzSLPbI"},"source":["# This functions takes a folder of files and returns one array with \n","# all of the files processed sentences(which themselves are a list of words) as elements \n","# def get_proc_docs_glove(training_paper_year, STARTWORDS, STOPWORDS, max_papers=None, verbose=True, use_porter=False, useStopWords=False):\n","#   global_path = \"/content/drive/MyDrive/regen_x/data/ocr_paper_COMPREHENSIVE/\"\n","#   folder_path = global_path + \"{}/\".format(training_paper_year)\n","#   print(folder_path) \n","#   file_paths = glob.glob(folder_path + \"*.docx\")\n","\n","#   print(\"Number of files: {}\".format(len(file_paths)))\n","#   if len(file_paths) == 0:\n","#     raise Exception(\"Folder has no files - maybe drive was not mounted?\")\n","#   ## -- Collecting Papers from Given Year -- ##\n","#   proc_docs = [] \n","\n","#   counter = 1\n","#   length = len(file_paths)\n","#   for f in file_paths:\n","#     doc = ' '.join(get_docx(f))\n","#     # proc_doc = [word for word in re.findall(r'\\w+', doc.lower()) if ((word in STARTWORDS) and (len(word) > 2) and (word not in STOPWORDS))]\n","    \n","#     proc_doc = [word for word in re.findall(r'\\w+', doc) if ((len(word) > 2))]\n","\n","#     if use_porter:\n","#       proc_doc = do_stemming(proc_doc)      \n","#     else:\n","#       proc_doc = do_lemmatizing(proc_doc)\n","\n","#     if useStopWords and use_porter:\n","#       proc_doc = [word for word in proc_doc if (word not in STEMMED_STOPWORDS)]\n","#     if useStopWords and not use_porter:\n","#       proc_doc = [word for word in proc_doc if (word not in LEMMATIZED_STOPWORDS)]\n","\n","\n","\n","#     proc_docs.append(proc_doc)\n","#     print(\"{}/{}\".format(counter, length))\n","#     counter += 1\n","\n","#     if max_papers != None:\n","#       if counter == max_papers+1:\n","#         break \n","\n","#   return proc_docs\n","\n","def get_proc_docs_glove(training_paper_year, STARTWORDS, STOPWORDS, max_papers=None, verbose=True, use_porter=False, useStopWords=False):\n","  global_path = \"/content/drive/MyDrive/regen_x/data/ocr_paper_COMPREHENSIVE/\"\n","  folder_path = global_path + \"{}/\".format(training_paper_year)\n","  print(folder_path) \n","  file_paths = glob.glob(folder_path + \"*.docx\")\n","\n","  print(\"Number of files: {}\".format(len(file_paths)))\n","  if len(file_paths) == 0:\n","    raise Exception(\"Folder has no files - maybe drive was not mounted?\")\n","  ## -- Collecting Papers from Given Year -- ##\n","  proc_docs = [] \n","\n","  counter = 1\n","  length = len(file_paths)\n","  for f in file_paths:\n","    doc = ' '.join(get_docx(f))\n","    # proc_doc = [word for word in re.findall(r'\\w+', doc.lower()) if ((word in STARTWORDS) and (len(word) > 2) and (word not in STOPWORDS))]\n","    \n","    proc_doc = [] \n","    \n","    if useStopWords:\n","      proc_doc = [word for word in re.findall(r'\\w+', doc) if ((len(word) > 2) and (word not in STOPWORDS))]\n","    else:\n","      proc_doc = [word for word in re.findall(r'\\w+', doc)]\n","\n","\n","    if use_porter:\n","      proc_doc = do_stemming(proc_doc)      \n","    else:\n","      proc_doc = do_lemmatizing(proc_doc)\n","\n","\n","\n","    proc_docs.append(proc_doc)\n","    print(\"{}/{}\".format(counter, length))\n","    counter += 1\n","\n","    if max_papers != None:\n","      if counter == max_papers+1:\n","        break \n","\n","  return proc_docs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lw6f2rWfZIW4"},"source":["# Gensim Model"]},{"cell_type":"code","metadata":{"id":"AOTLfdUjPjEu"},"source":["from gensim.models import Word2Vec"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Qh2npZiZjgH"},"source":["# Comparison"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7_c07ynjbCZW","executionInfo":{"status":"ok","timestamp":1635435407153,"user_tz":240,"elapsed":2,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}},"outputId":"0f1e7c1b-10ea-46c8-de9b-7ee00f7d3bb3"},"source":["import itertools \n","set(itertools.permutations([True, True, False, False], 2))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{(False, False), (False, True), (True, False), (True, True)}"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"I_C2-ktjZkBR"},"source":["def train_models_for_year(year, word):\n","  df = pd.DataFrame()\n","\n","  permutations = [(True, False), (True, True), (False, False), (False, True)]\n","  for p in permutations:\n","    proc_docs = get_proc_docs(year, STARTWORDS, STOPWORDS, verbose=True, use_porter=p[0], useStopWords=p[1])\n","    proc_docs_glove = get_proc_docs_glove(year, STARTWORDS, STOPWORDS, verbose=True, use_porter=p[0], useStopWords=p[1]) # don't split into sentences for GloVe\n","\n","    gensim_model = Word2Vec(sentences=proc_docs, min_count=1) \n","    glove_model = train_glove(proc_docs_glove) \n","\n","    pre = \"\"\n","    stop = \"\"\n","    if p[0] == False:\n","      pre = \"Lemmatization \"\n","    else:\n","      pre = \"Stemming \"\n","\n","    if p[1] == False:\n","      stop = \"*Stopwords Included* - \"\n","    else:\n","      stop = \"No Stopwords - \"\n","\n","    try:\n","      print(gensim_model.wv.most_similar(word, topn=20))\n","      df[pre + stop + \"Gensim\"] = [word_tuple[0] for word_tuple in gensim_model.wv.most_similar(word, topn=20)]\n","    except KeyError:\n","      df[pre + stop + \"Gensim\"] = [\"Word Not Found\"] * 20\n","\n","    try:\n","      print(glove_model.most_similar(word, number=21))\n","      df[pre + stop + \"Glove\"] = [word_tuple[0] for word_tuple in glove_model.most_similar(word, number=21)]\n","    except:\n","      df[pre + stop + \"Glove\"] = [\"Word Not Found\"] * 20\n","\n","  return df\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6B-Xsm7KOTeg","executionInfo":{"status":"ok","timestamp":1635436710202,"user_tz":240,"elapsed":461275,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}},"outputId":"201fa99b-fd47-4de5-ad17-fa0dc7041040"},"source":["df = train_models_for_year(1907, \"eye\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/regen_x/data/ocr_paper_COMPREHENSIVE/1907/\n","Number of files: 5\n","\t1/5\n","\t2/5\n","\t3/5\n","\t4/5\n","\t5/5\n","/content/drive/MyDrive/regen_x/data/ocr_paper_COMPREHENSIVE/1907/\n","Number of files: 5\n","1/5\n","2/5\n","3/5\n","4/5\n","5/5\n","Performing 30 training epochs with 4 threads\n","Epoch 0\n","Epoch 1\n","Epoch 2\n","Epoch 3\n","Epoch 4\n","Epoch 5\n","Epoch 6\n","Epoch 7\n","Epoch 8\n","Epoch 9\n","Epoch 10\n","Epoch 11\n","Epoch 12\n","Epoch 13\n","Epoch 14\n","Epoch 15\n","Epoch 16\n","Epoch 17\n","Epoch 18\n","Epoch 19\n","Epoch 20\n","Epoch 21\n","Epoch 22\n","Epoch 23\n","Epoch 24\n","Epoch 25\n","Epoch 26\n","Epoch 27\n","Epoch 28\n","Epoch 29\n","[('part', 0.9918349981307983), ('other', 0.9902467727661133), ('side', 0.9868088364601135), ('cornea', 0.9850528240203857), ('muscl', 0.9843302965164185), ('lesion', 0.9836300611495972), ('lower', 0.9828320741653442), ('involv', 0.9822753071784973), ('on', 0.9810366630554199), ('upper', 0.9804495573043823), ('sensori', 0.9785416126251221), ('motor', 0.977988600730896), ('posterior', 0.9775254726409912), ('cell', 0.9768990874290466), ('anterior', 0.9768800735473633), ('region', 0.9766694903373718), ('opposit', 0.9765570163726807), ('peripher', 0.9764447808265686), ('brain', 0.9761648178100586), ('posit', 0.9756346344947815)]\n","[('should', 0.9992885650741788), ('neuralgia', 0.9971043284753807), ('all', 0.9933338133739827), ('make', 0.9931686451325941), ('patient', 0.9924600294383139), ('with', 0.9918751429067917), ('ptosi', 0.9910090159027397), ('short', 0.9909842786616394), ('brain', 0.989701995389246), ('agitan', 0.9887719851217552), ('time', 0.9885257193292588), ('good', 0.987630451882151), ('degre', 0.9872436125494871), ('also', 0.9868851119785881), ('them', 0.9865749436091731), ('complic', 0.9862637805517778), ('wa', 0.985443536831195), ('repeat', 0.9846367524320727), ('trace', 0.984565187948151), ('could', 0.9844957529861444)]\n","/content/drive/MyDrive/regen_x/data/ocr_paper_COMPREHENSIVE/1907/\n","Number of files: 5\n","\t1/5\n","\t2/5\n","\t3/5\n","\t4/5\n","\t5/5\n","/content/drive/MyDrive/regen_x/data/ocr_paper_COMPREHENSIVE/1907/\n","Number of files: 5\n","1/5\n","2/5\n","3/5\n","4/5\n","5/5\n","Performing 30 training epochs with 4 threads\n","Epoch 0\n","Epoch 1\n","Epoch 2\n","Epoch 3\n","Epoch 4\n","Epoch 5\n","Epoch 6\n","Epoch 7\n","Epoch 8\n","Epoch 9\n","Epoch 10\n","Epoch 11\n","Epoch 12\n","Epoch 13\n","Epoch 14\n","Epoch 15\n","Epoch 16\n","Epoch 17\n","Epoch 18\n","Epoch 19\n","Epoch 20\n","Epoch 21\n","Epoch 22\n","Epoch 23\n","Epoch 24\n","Epoch 25\n","Epoch 26\n","Epoch 27\n","Epoch 28\n","Epoch 29\n","[('treatment', 0.9999457597732544), ('patient', 0.9999443292617798), ('report', 0.9999378323554993), ('oper', 0.9999377727508545), ('age', 0.9999369382858276), ('solut', 0.9999338388442993), ('cornea', 0.999932050704956), ('day', 0.9999237656593323), ('1', 0.9999235272407532), ('month', 0.9999222159385681), ('method', 0.9999221563339233), ('myopia', 0.9999215602874756), ('week', 0.9999178647994995), ('tion', 0.9999172687530518), ('vision', 0.9999166131019592), ('3', 0.9999158382415771), ('inject', 0.9999154210090637), ('2', 0.999915361404419), ('studi', 0.9999150037765503), ('paper', 0.9999135136604309)]\n","[('angioma', 0.9945104264041119), ('genev', 0.983185434927522), ('sound', 0.9818479303345703), ('enucl', 0.9766772948915535), ('sit', 0.9761613871465936), ('correct', 0.9717420300096892), ('myopic', 0.9712040396060221), ('experi', 0.9684562188103668), ('demonstr', 0.967465141663958), ('quiet', 0.9657676643587669), ('direct', 0.963269344939524), ('remov', 0.9594563851395961), ('phototyp', 0.955889064904319), ('lid', 0.9548479855801518), ('stephenson', 0.9529935399975554), ('requir', 0.9524712799239088), ('opaqu', 0.943767926364469), ('lacau', 0.9425454119206761), ('specimen', 0.9395531483643569), ('stanc', 0.9381016326175619)]\n","/content/drive/MyDrive/regen_x/data/ocr_paper_COMPREHENSIVE/1907/\n","Number of files: 5\n","\t1/5\n","\t2/5\n","\t3/5\n","\t4/5\n","\t5/5\n","/content/drive/MyDrive/regen_x/data/ocr_paper_COMPREHENSIVE/1907/\n","Number of files: 5\n","1/5\n","2/5\n","3/5\n","4/5\n","5/5\n","Performing 30 training epochs with 4 threads\n","Epoch 0\n","Epoch 1\n","Epoch 2\n","Epoch 3\n","Epoch 4\n","Epoch 5\n","Epoch 6\n","Epoch 7\n","Epoch 8\n","Epoch 9\n","Epoch 10\n","Epoch 11\n","Epoch 12\n","Epoch 13\n","Epoch 14\n","Epoch 15\n","Epoch 16\n","Epoch 17\n","Epoch 18\n","Epoch 19\n","Epoch 20\n","Epoch 21\n","Epoch 22\n","Epoch 23\n","Epoch 24\n","Epoch 25\n","Epoch 26\n","Epoch 27\n","Epoch 28\n","Epoch 29\n","[('part', 0.9903448224067688), ('lesion', 0.9892838597297668), ('side', 0.9887020587921143), ('motor', 0.9883334636688232), ('other', 0.9879796504974365), ('neuron', 0.9873130321502686), ('position', 0.9863366484642029), ('diagnosis', 0.982594907283783), ('cornea', 0.9822216629981995), ('brain', 0.9821798205375671), ('posterior', 0.9814034104347229), ('coagulability', 0.9813279509544373), ('muscle', 0.981151819229126), ('on', 0.9809859991073608), ('anterior', 0.9806289076805115), ('peripheral', 0.9806108474731445), ('root', 0.9800049662590027), ('papillo', 0.9797991514205933), ('upper', 0.9796407222747803), ('paralysis', 0.979436457157135)]\n","[('operation', 0.9962555930110673), ('sufficiently', 0.9955382984510004), ('semaphore', 0.9946667978610927), ('later', 0.9926835762413746), ('try', 0.9916427600625729), ('child', 0.9911591301183076), ('repeat', 0.9907652608197544), ('his', 0.988843555663647), ('use', 0.9870430524294896), ('instrument', 0.9868914928057374), ('necessary', 0.9854265108726499), ('for', 0.985415007692945), ('should', 0.9842697858411282), ('trace', 0.982495003895438), ('last', 0.9824805078892961), ('before', 0.9821406041078596), ('after', 0.9819465199508729), ('become', 0.9817806170901469), ('keratitis', 0.9814633216890252), ('see', 0.9811844887979425)]\n","/content/drive/MyDrive/regen_x/data/ocr_paper_COMPREHENSIVE/1907/\n","Number of files: 5\n","\t1/5\n","\t2/5\n","\t3/5\n","\t4/5\n","\t5/5\n","/content/drive/MyDrive/regen_x/data/ocr_paper_COMPREHENSIVE/1907/\n","Number of files: 5\n","1/5\n","2/5\n","3/5\n","4/5\n","5/5\n","Performing 30 training epochs with 4 threads\n","Epoch 0\n","Epoch 1\n","Epoch 2\n","Epoch 3\n","Epoch 4\n","Epoch 5\n","Epoch 6\n","Epoch 7\n","Epoch 8\n","Epoch 9\n","Epoch 10\n","Epoch 11\n","Epoch 12\n","Epoch 13\n","Epoch 14\n","Epoch 15\n","Epoch 16\n","Epoch 17\n","Epoch 18\n","Epoch 19\n","Epoch 20\n","Epoch 21\n","Epoch 22\n","Epoch 23\n","Epoch 24\n","Epoch 25\n","Epoch 26\n","Epoch 27\n","Epoch 28\n","Epoch 29\n","[('patient', 0.9999586343765259), ('report', 0.9999443888664246), ('cornea', 0.9999430179595947), ('age', 0.9999428987503052), ('treatment', 0.9999428987503052), ('operation', 0.9999407529830933), ('solution', 0.9999342560768127), ('3', 0.9999330043792725), ('paper', 0.999932587146759), ('lens', 0.9999318718910217), ('vision', 0.9999297857284546), ('1', 0.9999289512634277), ('myopia', 0.9999278783798218), ('tion', 0.999927282333374), ('lid', 0.9999239444732666), ('normal', 0.9999215006828308), ('injection', 0.9999212622642517), ('form', 0.9999207258224487), ('day', 0.9999191761016846), ('method', 0.9999186992645264)]\n","[('lapse', 0.9932982186031671), ('direction', 0.9874813164790649), ('application', 0.985981349928047), ('exciting', 0.983472857568634), ('chorea', 0.9822382724238411), ('subject', 0.9784590338231836), ('host', 0.9672017737052211), ('quiet', 0.9660755630617331), ('hausmann', 0.963787282879462), ('rosis', 0.9622751474989965), ('geon', 0.9556027890850017), ('normal', 0.9527060776672087), ('notice', 0.9515043920328518), ('painful', 0.9478709407062949), ('beriberi', 0.9463064746810487), ('hypermetropia', 0.9437381342340795), ('squinting', 0.9435265188682648), ('remain', 0.94087752894516), ('require', 0.9389699159571889), ('careful', 0.9386006148727514)]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":686},"id":"llYtyMPaey6P","executionInfo":{"status":"ok","timestamp":1635436745941,"user_tz":240,"elapsed":325,"user":{"displayName":"Varun Krishnan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10781844920353862932"}},"outputId":"e6ea26fb-592a-4066-82f1-027fdcc5767d"},"source":["df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Stemming *Stopwords Included* - Gensim</th>\n","      <th>Stemming *Stopwords Included* - Glove</th>\n","      <th>Stemming No Stopwords - Gensim</th>\n","      <th>Stemming No Stopwords - Glove</th>\n","      <th>Lemmatization *Stopwords Included* - Gensim</th>\n","      <th>Lemmatization *Stopwords Included* - Glove</th>\n","      <th>Lemmatization No Stopwords - Gensim</th>\n","      <th>Lemmatization No Stopwords - Glove</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>part</td>\n","      <td>should</td>\n","      <td>treatment</td>\n","      <td>angioma</td>\n","      <td>part</td>\n","      <td>operation</td>\n","      <td>patient</td>\n","      <td>lapse</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>other</td>\n","      <td>neuralgia</td>\n","      <td>patient</td>\n","      <td>genev</td>\n","      <td>lesion</td>\n","      <td>sufficiently</td>\n","      <td>report</td>\n","      <td>direction</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>side</td>\n","      <td>all</td>\n","      <td>report</td>\n","      <td>sound</td>\n","      <td>side</td>\n","      <td>semaphore</td>\n","      <td>cornea</td>\n","      <td>application</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>cornea</td>\n","      <td>make</td>\n","      <td>oper</td>\n","      <td>enucl</td>\n","      <td>motor</td>\n","      <td>later</td>\n","      <td>age</td>\n","      <td>exciting</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>muscl</td>\n","      <td>patient</td>\n","      <td>age</td>\n","      <td>sit</td>\n","      <td>other</td>\n","      <td>try</td>\n","      <td>treatment</td>\n","      <td>chorea</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>lesion</td>\n","      <td>with</td>\n","      <td>solut</td>\n","      <td>correct</td>\n","      <td>neuron</td>\n","      <td>child</td>\n","      <td>operation</td>\n","      <td>subject</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>lower</td>\n","      <td>ptosi</td>\n","      <td>cornea</td>\n","      <td>myopic</td>\n","      <td>position</td>\n","      <td>repeat</td>\n","      <td>solution</td>\n","      <td>host</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>involv</td>\n","      <td>short</td>\n","      <td>day</td>\n","      <td>experi</td>\n","      <td>diagnosis</td>\n","      <td>his</td>\n","      <td>3</td>\n","      <td>quiet</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>on</td>\n","      <td>brain</td>\n","      <td>1</td>\n","      <td>demonstr</td>\n","      <td>cornea</td>\n","      <td>use</td>\n","      <td>paper</td>\n","      <td>hausmann</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>upper</td>\n","      <td>agitan</td>\n","      <td>month</td>\n","      <td>quiet</td>\n","      <td>brain</td>\n","      <td>instrument</td>\n","      <td>lens</td>\n","      <td>rosis</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>sensori</td>\n","      <td>time</td>\n","      <td>method</td>\n","      <td>direct</td>\n","      <td>posterior</td>\n","      <td>necessary</td>\n","      <td>vision</td>\n","      <td>geon</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>motor</td>\n","      <td>good</td>\n","      <td>myopia</td>\n","      <td>remov</td>\n","      <td>coagulability</td>\n","      <td>for</td>\n","      <td>1</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>posterior</td>\n","      <td>degre</td>\n","      <td>week</td>\n","      <td>phototyp</td>\n","      <td>muscle</td>\n","      <td>should</td>\n","      <td>myopia</td>\n","      <td>notice</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>cell</td>\n","      <td>also</td>\n","      <td>tion</td>\n","      <td>lid</td>\n","      <td>on</td>\n","      <td>trace</td>\n","      <td>tion</td>\n","      <td>painful</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>anterior</td>\n","      <td>them</td>\n","      <td>vision</td>\n","      <td>stephenson</td>\n","      <td>anterior</td>\n","      <td>last</td>\n","      <td>lid</td>\n","      <td>beriberi</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>region</td>\n","      <td>complic</td>\n","      <td>3</td>\n","      <td>requir</td>\n","      <td>peripheral</td>\n","      <td>before</td>\n","      <td>normal</td>\n","      <td>hypermetropia</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>opposit</td>\n","      <td>wa</td>\n","      <td>inject</td>\n","      <td>opaqu</td>\n","      <td>root</td>\n","      <td>after</td>\n","      <td>injection</td>\n","      <td>squinting</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>peripher</td>\n","      <td>repeat</td>\n","      <td>2</td>\n","      <td>lacau</td>\n","      <td>papillo</td>\n","      <td>become</td>\n","      <td>form</td>\n","      <td>remain</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>brain</td>\n","      <td>trace</td>\n","      <td>studi</td>\n","      <td>specimen</td>\n","      <td>upper</td>\n","      <td>keratitis</td>\n","      <td>day</td>\n","      <td>require</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>posit</td>\n","      <td>could</td>\n","      <td>paper</td>\n","      <td>stanc</td>\n","      <td>paralysis</td>\n","      <td>see</td>\n","      <td>method</td>\n","      <td>careful</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Stemming *Stopwords Included* - Gensim  ... Lemmatization No Stopwords - Glove\n","0                                    part  ...                              lapse\n","1                                   other  ...                          direction\n","2                                    side  ...                        application\n","3                                  cornea  ...                           exciting\n","4                                   muscl  ...                             chorea\n","5                                  lesion  ...                            subject\n","6                                   lower  ...                               host\n","7                                  involv  ...                              quiet\n","8                                      on  ...                           hausmann\n","9                                   upper  ...                              rosis\n","10                                sensori  ...                               geon\n","11                                  motor  ...                             normal\n","12                              posterior  ...                             notice\n","13                                   cell  ...                            painful\n","14                               anterior  ...                           beriberi\n","15                                 region  ...                      hypermetropia\n","16                                opposit  ...                          squinting\n","17                               peripher  ...                             remain\n","18                                  brain  ...                            require\n","19                                  posit  ...                            careful\n","\n","[20 rows x 8 columns]"]},"metadata":{},"execution_count":38}]}]}